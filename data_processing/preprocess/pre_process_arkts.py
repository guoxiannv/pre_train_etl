import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
from utils import *
import re
from typing import Dict, List, Optional
import hashlib
import json
from collections import defaultdict
from datasketch import MinHash, MinHashLSH
import tree_sitter_typescript as tst
from tree_sitter import Language, Parser
from tqdm import tqdm

# --- Part 1: Heuristic Filtering ---

# Configuration for heuristic-based filtering rules.
# Thresholds are based on best practices from projects like The Stack and Codex.
SEED = 42
MAX_AVG_LINE_LENGTH = 100
MAX_LINE_LENGTH = 1000
AUTO_GEN_KEYWORDS = [
    "auto-generated",
    # "autogenerated",
    # "do not edit",
    # "generated by",
]

tree_sitter_language = Language(tst.language_typescript())
parser = Parser()
parser.language = tree_sitter_language


def mask_private_key(text: str) -> str:
    """
    将 PEM 格式的私钥段内容替换成 <MASKED_PRIVATE_KEY> 占位符。
    支持 BEGIN/END PRIVATE KEY 以及 BEGIN/END RSA PRIVATE KEY。
    """
    pattern = (
        r'-----BEGIN (?:RSA )?PRIVATE KEY-----'  # 开头
        r'.*?'                                   # 任意内容（包括换行）
        r'-----END (?:RSA )?PRIVATE KEY-----'    # 结束
    )
    return re.sub(
        pattern,
        lambda m: "-----BEGIN PRIVATE KEY-----\n<MASKED_PRIVATE_KEY>\n-----END PRIVATE KEY-----",
        text,
        flags=re.DOTALL | re.IGNORECASE
    )

def auto_generated_filter(text: str) -> bool:
    """Filter out auto-generated files"""
    if not text:
        return False
    lower_content = text.lower()
    return not any(keyword in lower_content for keyword in AUTO_GEN_KEYWORDS)


def avg_line_length_filter(text: str, max_avg_length: int = MAX_AVG_LINE_LENGTH) -> bool:
    """Filter out records with excessive average line length"""
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    total_chars = sum(len(line) for line in lines)
    return (total_chars / len(lines)) <= max_avg_length


def max_line_length_filter(text: str, max_length: int = MAX_LINE_LENGTH) -> bool:
    """Filter out records with extremely long lines"""
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    return not any(len(line) > max_length for line in lines)


# --- Tree-sitter syntax error filter ---

def _count_ts_errors_and_missing(text: str):
    """
    统计 Tree-sitter 解析树中的 ERROR 节点数量与 missing 节点数量。
    返回: (error_nodes, missing_nodes, total_nodes)
    """
    if not text or not isinstance(text, str):
        return (0, 0, 0)

    tree = parser.parse(bytes(text, "utf8"))
    root = tree.root_node
    if root is None:
        return (0, 0, 0)

    err_cnt = 0
    miss_cnt = 0
    total = 0

    # DFS 遍历
    stack = [root]
    while stack:
        node = stack.pop()
        total += 1

        # 明确 ERROR 类型的节点
        if node.type == "ERROR":
            err_cnt += 1
        # 语法中缺失的必需构件
        if node.is_missing:
            miss_cnt += 1

        # 避免重复计算：has_error 只作为快速路径，不单独计数
        # if node.has_error: pass

        # 推入子节点
        # 用 children 而非 named_children，确保不漏掉结构性 token
        stack.extend(node.children)

    return (err_cnt, miss_cnt, total)


def tree_sitter_error_filter(
    text: str,
    max_error_nodes: int = 0,
    allow_missing: bool = False,
    max_error_ratio: float = 0.0,
):
    """
    语法错误过滤器：
    - 若 ERROR 节点数 > max_error_nodes，则过滤（返回 False）
    - 若不允许缺失且存在 missing 节点，则过滤
    - 若设置了 max_error_ratio 且 ERROR 节点占比超过阈值，也过滤
    返回 True 表示“保留”，False 表示“过滤掉”
    """
    err_cnt, miss_cnt, total = _count_ts_errors_and_missing(text)

    # 缺失节点控制
    if not allow_missing and miss_cnt > 0:
        return False

    # 绝对数量阈值
    if err_cnt > max_error_nodes:
        return False

    # 占比阈值（可选）
    if total > 0 and max_error_ratio > 0.0:
        if (err_cnt / total) > max_error_ratio:
            return False

    return True


def _normalize_for_syntax(text: str) -> str:
    """
    仅用于语法检查的标准化：
    - 处理常见转义序列（\\n/\\t/\\r/\"/\'/\\）
    - 统一换行符为 \n
    不改变原始文本，只返回用于 parse 的副本。
    """
    if not text or not isinstance(text, str):
        return ""

    normalized = text
    # 常见转义序列
    escape_mappings = {
        r"\\n": "\n",
        r"\\t": "\t",
        r"\\r": "\r",
        r"\\\"": '"',
        r"\\'": "'",
        r"\\\\": "\\",
    }
    for escaped, unescaped in escape_mappings.items():
        normalized = normalized.replace(escaped, unescaped)

    # 处理其他形式的单字符转义
    normalized = re.sub(r"\\(.)", r"\1", normalized)

    # 统一换行
    normalized = normalized.replace("\r\n", "\n").replace("\r", "\n")

    return normalized


def arkts_syntax_filter(text: str, max_errors: int = 0, allow_missing: bool = False):
    """
    严格语法校验（参考 syntax_check_filter.py）：
    - 只要存在任何 ERROR 节点或 missing 节点（当 allow_missing=False）即判为失败
    - 返回 (passed: bool, err_cnt: int, miss_cnt: int, total_nodes: int)
    """
    if not text or not isinstance(text, str):
        return False, 0, 0, 0

    norm = _normalize_for_syntax(text)
    err_cnt, miss_cnt, total_nodes = _count_ts_errors_and_missing(norm)

    if not allow_missing and miss_cnt > 0:
        return False, err_cnt, miss_cnt, total_nodes
    if err_cnt > max_errors:
        return False, err_cnt, miss_cnt, total_nodes
    return True, err_cnt, miss_cnt, total_nodes

def empty_content_filter(text: str) -> bool:
    """Filter out records with no content"""
    if not text:
        return False
    lines = text.splitlines()
    return bool(lines)


# --- Part 2: Exact Deduplication ---

def get_content_hash(content):
    """
    Computes the SHA256 hash of a string content.

    Args:
        content (str): The string content to hash.

    Returns:
        str: The hexadecimal SHA256 hash digest.
    """
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def find_exact_duplicates(records):
    """
    Finds records with exactly duplicate content using hashing.

    Args:
        records (list[dict]): A list of records to check for duplicates.
                               Each record must have 'id' and 'content'.

    Returns:
        set: A set of 'id's of the records to be removed.
    """
    hashes = defaultdict(list)
    for record in records:
        content_hash = get_content_hash(record['text'])
        hashes[content_hash].append(record['id'])

    to_remove = set()
    for hash_val, ids in hashes.items():
        if len(ids) > 1:
            # Keep the first one, mark the rest for removal
            to_remove.update(ids[1:])
            print(f"  - Found {len(ids)} exact duplicates. Keeping '{ids}', removing {ids[1:]}.")

    return to_remove


# --- Part 3: Near-Deduplication ---

# Configuration for MinHash and LSH parameters.
# Based on industry standards and recommendations from The Stack.
JACCARD_THRESHOLD = 0.85
NUM_PERMUTATIONS = 256


def find_near_duplicates(records):
    """
    Finds near-duplicate records using MinHash and LSH.

    Args:
        records (list[dict]): A list of records to check.

    Returns:
        set: A set of 'id's of the near-duplicate records to be removed.
    """
    print("\n Creating MinHash signatures...")

    # 1. Create a MinHash signature for each record
    minhashes = {}
    for i, record in enumerate(records):
        content = record.get("text", "")
        if not content:
            continue

        # Create shingles (5-grams of characters) from the content
        shingles = {content[i:i + 5] for i in range(len(content) - 4)}
        if not shingles:
            continue

        m = MinHash(num_perm=NUM_PERMUTATIONS, seed=SEED)
        for s in shingles:
            m.update(s.encode('utf8'))
        minhashes[record['id']] = m

        if (i + 1) % 200 == 0:
            print(f"  - Processed {i + 1}/{len(records)} records...")

    print(f" MinHash signatures created for {len(minhashes)} records.")
    print(" Building LSH index...")

    # 2. Index all MinHash signatures in the LSH model
    lsh = MinHashLSH(threshold=JACCARD_THRESHOLD, num_perm=NUM_PERMUTATIONS)
    for record_id, m in minhashes.items():
        lsh.insert(record_id, m)

    print(" LSH index built. Querying for duplicate clusters...")

    # 3. Query for duplicates and identify clusters
    processed_ids = set()
    clusters = []
    for record_id, m in minhashes.items():
        if record_id in processed_ids:
            continue

        # Find candidate pairs for the current record
        result = lsh.query(m)

        if len(result) > 1:
            cluster = set(result)
            clusters.append(list(cluster))
            processed_ids.update(cluster)

    # 4. From each cluster, keep one record and mark others for removal
    to_remove = set()
    for cluster in clusters:
        # A simple strategy: keep the record with the lexicographically smallest ID
        cluster.sort()
        kept_id = cluster[0]
        removed_ids = cluster[1:]
        to_remove.update(removed_ids)

        print(f"  - Cluster found. Keeping '{kept_id}', removing {removed_ids}")

    return to_remove

def remove_angle_bracket_headers(text: str) -> str:
    """
    Remove all lines containing <any content> format from the beginning of the code.
    Only processes consecutive lines containing angle bracket tags at the start of the file.
    
    Args:
        text: String containing the code
        
    Returns:
        Code string after removing all <any content> header information
    """
    lines = text.split('\n')
    # Find index of first line not containing <tag> format
    start_index = 0
    for i, line in enumerate(lines):
        # Check if line contains <tag> format (excluding cases within quotes)
        # Simple heuristic: if line starts with < or main content is <tag>, consider it header info
        stripped_line = line.strip()
        if not stripped_line:
            continue
        # If line contains <tag> and looks like header info (no common code syntax)
        if ('<' in line and '>' in line and 
            not any(keyword in line for keyword in ['=', '(', ')', '{', '}', ';', 'function', 'const', 'let', 'var', 'class', 'import', 'export']) and
            not line.strip().startswith('//')): # Not a comment
            start_index = i + 1
        else:
            break
    
    return '\n'.join(lines[start_index:])


def clean_code_headers(code: str) -> str:
    """
    Uses regular expressions to remove Copyright and Licensed comment blocks from the code.

    Args:
        code: A string containing the code.

    Returns:
        The code string after removing the copyright and license statements.
    """
    # This regular expression finds multi-line comment blocks (/* ... */)
    # containing the keywords "Copyright" or "Licensed"
    # The re.DOTALL flag allows '.' to match any character, including newlines.
    # The re.IGNORECASE flag makes the matching case-insensitive.
    # \s* will match and remove trailing whitespace (like newlines) after the comment block for cleaner formatting.
    pattern = r'/\*+.*?(?:Copyright|Licensed).*?\*/\s*'

    cleaned_code = re.sub(pattern, '', code, flags=re.DOTALL | re.IGNORECASE)

    return cleaned_code.lstrip()  # Remove potential leading blank lines created by deleting the comment.

def remove_single_line_comments(text: str) -> str:
    """Strip // comments"""
    return re.sub(r'//.*', '', text)

def remove_leading_blank_lines(text: str) -> str:
    """
    Remove blank lines (empty or whitespace-only) at the very beginning of the text.
    """
    if not isinstance(text, str) or not text:
        return text
    lines = text.split('\n')
    start_index = 0
    while start_index < len(lines) and lines[start_index].strip() == '':
        start_index += 1
    return '\n'.join(lines[start_index:])

def ensure_ets_header(text: str, path: Optional[str]) -> str:
    """
    Ensure the first non-empty line is a header like "// dir/file.ets".
    If missing and `path` is provided, prepend a header built from the last two segments of `path`.
    The file name is normalized to have a .ets extension.
    """
    if not isinstance(text, str):
        return text

    header_regex = re.compile(r'^\s*//\s*.+\.ets\s*$', re.IGNORECASE)
    lines = text.split('\n')

    # Find first non-empty line
    first_idx = 0
    while first_idx < len(lines) and lines[first_idx].strip() == '':
        first_idx += 1

    if first_idx < len(lines) and header_regex.match(lines[first_idx] or ''):
        return text

    if not path:
        return text

    norm_path = os.path.normpath(path)
    parts = [p for p in norm_path.split(os.sep) if p]
    if not parts:
        return text
    last_two = '/'.join([parts[-1]]) if len(parts) >= 2 else parts[-1]
    header_line = f"// {last_two}"
    return header_line + ('\n' if text and not text.startswith('\n') else '\n') + text

# --- New Filter Functions ---

def min_line_count_filter(text: str, min_lines: int = 5) -> bool:
    """Keep if text has at least min_lines non-empty lines"""
    content_lines = get_content_lines(text)
    return len(content_lines) >= min_lines


def blank_vs_content_filter(text: str, max_ratio: float = 0.8, min_blank: int = 14) -> bool:
    """Filter out if blank lines exceed content lines or ratio threshold"""
    lines = text.splitlines()
    content_lines = [l for l in lines if l.strip()]
    blank = len(lines) - len(content_lines)
    if blank > len(content_lines):
        return False
    if len(content_lines) > 0 and (blank / len(content_lines) > max_ratio and blank >= min_blank):
        return False
    return True


def colon_density_filter(text: str, threshold: float = 0.45, min_content: int = 80) -> bool:
    """Filter out ArkTS-heavy records with high colon density"""
    content_lines = [l for l in text.splitlines() if l.strip()]
    if len(content_lines) < min_content:
        return True
    density = sum(1 for l in content_lines if ':' in l) / len(content_lines)
    return density <= threshold


def trivial_assignment_filter(text: str, vars: List[str] = ['a','b','c']) -> bool:
    """Drop if trivial assignments like 'a =', 'b=' etc. appear"""
    pattern = rf"\b({'|'.join(vars)})\s*="
    return re.search(pattern, text) is None


def get_content_lines(text: str) -> List[str]:
    """Extract non-empty, non-comment lines from text"""
    return [line.strip() for line in text.splitlines() 
            if line.strip() and not line.strip().startswith('//')]

def prefix_repetition_filter(text: str, prefix_len: int = 16, max_density: float = 0.4) -> bool:
    """Ensure no prefix of length prefix_len repeats more than max_density ratio of content lines"""
    # Get non-empty, non-comment lines
    content_lines = get_content_lines(text)
    if not content_lines:
        return False
        
    # Count prefix occurrences
    counts = defaultdict(int)
    for line in content_lines:
        if len(line) <= prefix_len:
            continue
        prefix = line[:prefix_len]
        counts[prefix] += 1
    
    # Check if any prefix density exceeds threshold
    total_lines = len(content_lines)
    for count in counts.values():
        if count / total_lines > max_density:
            return False
            
    return True


def chinese_char_threshold_filter(text: str, threshold: float = 0.3) -> bool:
    """Drop if Chinese character ratio exceeds threshold"""
    # Get content lines to calculate valid characters
    content_lines = get_content_lines(text)
    if not content_lines:
        return True
    
    # Calculate total valid characters (excluding whitespace)
    total_chars = sum(len(line.replace(' ', '').replace('\t', '')) for line in content_lines)
    if total_chars == 0:
        return True
    
    # Count Chinese characters
    chinese_count = sum(1 for c in ''.join(content_lines) if '\u4e00' <= c <= '\u9fff')
    chinese_ratio = chinese_count / total_chars
    
    return chinese_ratio <= threshold


def forbidden_token_frequency_filter(text: str, limits: Optional[Dict[str, int]] = None) -> bool:
    """Drop if any forbidden token appears more than allowed times"""
    if limits is None:
        limits = {'public': 6, 'private': 6}
    return not any(text.count(tok) > thr for tok, thr in limits.items())

def numeric_literal_density_filter(text: str, max_density: float = 0.3) -> bool:
    """
    如果数值常量在所有 token 中占比超过 max_density，则过滤掉该 record。
    """
    # 匹配所有数值常量：整数或浮点数
    num_literals = re.findall(r'\b-?\d+(\.\d+)?\b', text)
    # 匹配所有单词 token（包括标识符、关键字、常量等）
    all_tokens = re.findall(r'\b\w+\b', text)
    if not all_tokens:
        return False
    density = len(num_literals) / len(all_tokens)
    return density <= max_density

def numeric_array_length_filter(text: str, max_len: int = 10) -> bool:
    """
    检测代码中所有 [...] 数组初始化：
    若数组内所有元素均为数值且元素数量 > max_len，则过滤掉该 record。
    """
    for match in re.finditer(r'\[([^\]]+)\]', text):
        elems = [e.strip() for e in match.group(1).split(',') if e.strip()]
        # 判断是否全部为数字字面量
        if elems and all(re.fullmatch(r'-?\d+(\.\d+)?', e) for e in elems):
            if len(elems) > max_len:
                return False
    return True

def total_length_filter(text: str, max_length: int = 30696) -> bool:
    """Filter out records with total character count exceeding max_length"""
    return len(text) <= max_length


def count_empty_functions(text: str):
    """
    返回 (total_functions, empty_functions)
    空函数定义为：函数体里只有大括号，没有任何子节点语句。
    """
    tree = parser.parse(bytes(text, 'utf8'))
    root = tree.root_node

    if root is None:
        return 0, 0

    total, empty = 0, 0
    # Tree-sitter 中常见的函数/方法节点类型，视语言而定可增删
    fn_types = {
        'function_declaration',
        'method_definition',
        'function',
        'arrow_function',
        'function_signature'  # 根据你用的 TypeScript 语法树可能要调整
    }

    # 使用显式栈进行 DFS，避免 cursor.node 的可空类型告警
    stack = [root]
    while stack:
        node = stack.pop()
        if getattr(node, 'type', None) in fn_types:
            total += 1
            # 找到直接子节点里的 `{ ... }`
            for c in getattr(node, 'children', []):
                if getattr(c, 'type', None) == 'statement_block':
                    non_brace_children = [
                        gc for gc in getattr(c, 'children', [])
                        if getattr(gc, 'type', None) not in ('{', '}')
                    ]
                    if len(non_brace_children) == 0:
                        empty += 1
                    break
        # 推入子节点
        stack.extend(getattr(node, 'children', []))

    return total, empty


def empty_function_density_filter(text: str,
                                  max_empty: int = 5,
                                  max_ratio: float = 0.3) -> bool:
    """
    空函数数 > max_empty 或 空函数/总函数 > max_ratio 时，返回 False（即过滤掉）。
    """
    total, empty = count_empty_functions(text)
    if total == 0:
        return True  # 没有函数，不走这条规则
    if empty > max_empty or (empty / total) > max_ratio:
        return False
    return True

# 预编译若干常见模板语法的正则
_RE_JINJA_STMT = re.compile(r"{%-?\s*[^%]+?-?%}")                 # {% ... %}
_RE_JINJA_EXPR = re.compile(r"{{-?\s*[^}]+?-?}}")                 # {{ ... }}
_RE_JINJA_CTRL = re.compile(r"{%-?\s*(for|if|elif|else|endif|endfor|block|endblock|extends|include|set|macro|endmacro)\b[^%]*-?%}")

_RE_MUSTACHE_SEC = re.compile(r"{{\s*[#/\^>]\s*[^}]+}}")          # {{#section}}, {{/section}}, {{^inverse}}, {{> partial}}
_RE_HANDLEBARS_TRIPLE = re.compile(r"{{{[^}]+}}}")                # {{{ ... }}}

_RE_EJS_ERB = re.compile(r"<%[\s\S]*?%>")                         # EJS/ERB <% ... %>

# 你示例中的“/*--- ... ---*/”块（YAML风注释）也作为信号
_RE_YAML_BLOCK_COMMENT = re.compile(r"/\*\s*---[\s\S]+?---\s*\*/")

def jinja_template_filter(code: str) -> bool:
    """
    检测代码片段是否包含常见模板语法（Jinja/Nunjucks/Mustache/Handlebars/EJS/ERB 等）。
    返回 True 表示“应过滤（疑似模板）”；False 表示“保留”。
    """
    if not code or not isinstance(code, str):
        return False

    # 高置信度命中：任何一种即可直接过滤
    if (_RE_JINJA_STMT.search(code) or
        _RE_JINJA_CTRL.search(code) or
        _RE_EJS_ERB.search(code) or
        _RE_MUSTACHE_SEC.search(code) or
        _RE_HANDLEBARS_TRIPLE.search(code) or
        _RE_YAML_BLOCK_COMMENT.search(code)):
        return True

    # 中等置信度：大量 {{ ... }} 表达式（但避免把普通语言误杀）
    # ArkTS/TS/JS 正常不会在代码里使用 {{}}（模板字符串是 ${}），
    # 若出现多处 {{...}}，可认为是模板。
    exprs = _RE_JINJA_EXPR.findall(code)
    if len(exprs) >= 2:
        return True

    # 低置信度补充：同时出现 {% 和 %} 也很可疑
    if code.count("{%") >= 1 and code.count("%}") >= 1:
        return True

    return False


def data_clean_pipeline(corpus: List[Dict], out_dir: str = "./filter_logs", preprocess_only: bool = False):
    """
    Cleans the input corpus by removing comments, filtering out empty texts,
    and ensuring that the text does not contain more than 5 Chinese characters.

    Args:
        corpus: A list of dictionaries, each containing a 'text' key with the code to be cleaned.
        out_dir: Directory to save filter logs.
        preprocess_only: If True, only run preprocessing steps (mask private keys, remove headers, clean code headers)
                        without applying filters and deduplication.

    Returns:
        A list of cleaned dictionaries.
    """
    os.makedirs(out_dir, exist_ok=True)
    removed = defaultdict(list)
    syntax_error_details = []  # 收集语法错误的详细信息
    print("--- Step 0: Begin with {} records ---".format(len(corpus)))
    tmp_corpus = []

    ## Preprocess and normalize headers
    for item in corpus:
        text = item.get("text", "")
        # 1) Mask PrivateKey
        text = mask_private_key(text)
        # 2) Trim leading blank lines
        text = remove_leading_blank_lines(text)
        # 3) Remove angle bracket headers at the very beginning
        text = remove_angle_bracket_headers(text)
        # 4) Remove license/copyright blocks
        text = clean_code_headers(text)
        # 5) Trim leading blank lines again in case previous step introduced them
        text = remove_leading_blank_lines(text)
        # 6) Ensure // path/to/file.ets header exists using last two segments of path
        text = ensure_ets_header(text, item.get("path"))

        item['text'] = text
        tmp_corpus.append(item)

    # If preprocess_only is True, skip all filtering and deduplication steps
    if preprocess_only:
        print("--- Preprocessing Only Mode: Skipping filters and deduplication ---")
        print(f"Returning {len(tmp_corpus)} preprocessed records.")
        return tmp_corpus

    print("--- Step 1: All Filters ---")
    aft = []
    for r in tmp_corpus:
        t = r['text']
        
        # Heuristic filters (拆分后的独立过滤器)
        if not empty_content_filter(t):
            removed['empty_content_filter'].append(r)
            continue
        if not auto_generated_filter(t):
            removed['auto_generated_filter'].append(r)
            continue
        if not avg_line_length_filter(t):
            removed['avg_line_length_filter'].append(r)
            continue
        if not max_line_length_filter(t):
            removed['max_line_length_filter'].append(r)
            continue
        # Additional filters
        # Tree-sitter 严格语法过滤：只要存在任何 ERROR 或 missing（不允许）即过滤
        passed, err_cnt, miss_cnt, total_nodes = arkts_syntax_filter(t)
        if not passed:
            removed['tree_sitter_error_filter'].append(r)
            syntax_error_details.append({
                'id': r.get('id'),
                'corpusid': r.get('corpusid', ''),
                'error_count': err_cnt,
                'missing_count': miss_cnt,
                'total_nodes': total_nodes,
                'error_type': '语法错误',
                'text_preview': (t[:200] + '...') if isinstance(t, str) and len(t) > 200 else t
            })
            continue
        if not min_line_count_filter(t):
            removed['min_line_count_filter'].append(r)
            continue
        if not blank_vs_content_filter(t):
            removed['blank_vs_content_filter'].append(r)
            continue
        if not colon_density_filter(t):
            removed['colon_density_filter'].append(r)
            continue
        if not trivial_assignment_filter(t):
            removed['trivial_assignment_filter'].append(r)
            continue
        if not prefix_repetition_filter(t, prefix_len=16):
            removed['prefix_repetition_filter'].append(r)
            continue
        if not chinese_char_threshold_filter(t):
            removed['chinese_char_threshold_filter'].append(r)
            continue
        if not forbidden_token_frequency_filter(t):
            removed['forbidden_token_frequency_filter'].append(r)
            continue
        if not numeric_literal_density_filter(t):
            removed['numeric_literal_density_filter'].append(r)
            continue
        if not numeric_array_length_filter(t):
            removed['numeric_array_length_filter'].append(r)
            continue
        if not total_length_filter(t):
            removed['total_length_filter'].append(r)
            continue
        if not empty_function_density_filter(t):
            removed['empty_function_density_filter'].append(r)
            continue
        # if not jinja_template_filter(t):
        #     removed['jinja_template_filter'].append(r)
        #     continue
        aft.append(r)

    # 分别写入每个 filter 丢弃记录
    for name, recs in {
        "removed_empty_content_filter": removed['empty_content_filter'],
        "removed_auto_generated_filter": removed['auto_generated_filter'],
        "removed_avg_line_length_filter": removed['avg_line_length_filter'],
        "removed_max_line_length_filter": removed['max_line_length_filter'],
        "removed_min_line_count_filter": removed['min_line_count_filter'],
        "removed_blank_vs_content_filter": removed['blank_vs_content_filter'],
        "removed_colon_density_filter": removed['colon_density_filter'],
        "removed_trivial_assignment_filter": removed['trivial_assignment_filter'],
        "removed_prefix_repetition_filter": removed['prefix_repetition_filter'],
        "removed_chinese_char_threshold_filter": removed['chinese_char_threshold_filter'],
        "removed_forbidden_token_frequency_filter": removed['forbidden_token_frequency_filter'],
        "removed_numeric_literal_density_filter": removed['numeric_literal_density_filter'],
        "removed_numeric_array_length_filter": removed['numeric_array_length_filter'],
        "removed_total_length_filter": removed['total_length_filter'],
        "removed_empty_function_density_filter": removed['empty_function_density_filter'],
        # "removed_jinja_template_filter": removed['jinja_template_filter'],
        # "removed_tree_sitter_error_filter": removed['tree_sitter_error_filter'],
    }.items():
        write_jsonl(recs, os.path.join(out_dir, f"{name}.jsonl"))
        print(f"{name} 丢弃 {len(recs)} 条")

    # 额外输出语法错误详情报告
    if syntax_error_details:
        details_path = os.path.join(out_dir, "removed_tree_sitter_error_details.jsonl")
        write_jsonl(syntax_error_details, details_path)
        print(f"removed_tree_sitter_error_details 记录 {len(syntax_error_details)} 条 -> {details_path}")

    print(f"Kept {len(aft)} after all filters.\n")

    # Step 2: Exact Deduplication
    print("--- Step 2: Applying Exact Deduplication ---")
    exact_dup_ids = find_exact_duplicates(aft)
    records_after_exact_dedup = [r for r in aft if r['id'] not in exact_dup_ids]
    print(f"Exact deduplication complete. Kept {len(records_after_exact_dedup)} records.\n")

    # Step 3: Near-Deduplication
    print("--- Step 3: Applying Near-Deduplication ---")
    near_dup_ids = find_near_duplicates(records_after_exact_dedup)
    final_clean_records = [r for r in records_after_exact_dedup if r['id'] not in near_dup_ids]
    print(f"Near-deduplication complete. Kept {len(final_clean_records)} records.\n")

    return final_clean_records



def process_file_in_batches(file_path: str, batch_size: int = 500):
    """
    分批处理大文件，避免内存溢出
    
    Args:
        file_path: 文件路径
        batch_size: 批次大小，默认500条记录
        
    Returns:
        处理后的记录列表
    """
    processed_records = []
    total_lines = 0
    processed_lines = 0
    
    print(f"   开始分批处理，批次大小: {batch_size}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            batch = []
            batch_count = 0
            
            for line_num, line in enumerate(f, 1):
                total_lines += 1
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    record = json.loads(line)
                    batch.append(record)
                    processed_lines += 1
                    
                    # 当批次满了，处理这一批
                    if len(batch) >= batch_size:
                        batch_count += 1
                        print(f"   处理批次 {batch_count}: {len(batch)} 条记录")
                        
                        # 处理当前批次
                        cleaned_batch = data_clean_pipeline(batch, preprocess_only=True)
                        processed_records.extend(cleaned_batch)
                        
                        print(f"   批次 {batch_count} 完成: {len(batch)} -> {len(cleaned_batch)} 条")
                        
                        # 清理批次内存
                        del batch
                        batch = []
                        
                        # 每处理几个批次后显示进度
                        if batch_count % 5 == 0:
                            print(f"   📊 已处理 {batch_count} 个批次，累计记录: {len(processed_records)}")
                        
                except json.JSONDecodeError as e:
                    if line_num <= 10:  # 只在前10行显示错误
                        print(f"   警告：第 {line_num} 行JSON格式错误: {e}")
                    continue
                except Exception as e:
                    if line_num <= 10:
                        print(f"   警告：第 {line_num} 行处理错误: {e}")
                    continue
        
        # 处理最后一批
        if batch:
            batch_count += 1
            print(f"   处理最后批次 {batch_count}: {len(batch)} 条记录")
            cleaned_batch = data_clean_pipeline(batch, preprocess_only=True)
            processed_records.extend(cleaned_batch)
            print(f"   最后批次完成: {len(batch)} -> {len(cleaned_batch)} 条")
            del batch
        
        print(f"   📊 分批处理完成: 总行数 {total_lines}, 有效记录 {processed_lines}, 处理后 {len(processed_records)} 条")
        
    except Exception as e:
        print(f"   ❌ 分批处理文件失败: {e}")
        import traceback
        traceback.print_exc()
    
    return processed_records


# --- Test Cases ---
if __name__ == "__main__":
    # Get the current working directory where the script is run from
    curdir = os.path.dirname(__file__)
    print(f"Running from directory: {curdir}")
    
    # 定义输入文件列表
    input_files = [
        "../code_data/raw_data/harmony_samples_formated.jsonl",
        "../code_data/cleaned_data/arkui_2k_pretrain_cleaned_formated.jsonl", 
        "../docs_data/pure_code.jsonl",
        "../code_data/raw_data/dz5484.jsonl"
    ]
    
    # 可配置的批次大小，根据内存情况调整
    BATCH_SIZE = 500  # 可以根据内存情况调整：内存小用200-300，内存大用500-1000
    
    print(f"💾 批次大小配置: {BATCH_SIZE} 条记录/批次")
    print(f"💡 如果内存不足，可以减小这个值")
    
    # 逐个文件处理，避免内存溢出
    all_cleaned_data = []
    total_processed = 0
    
    for i, file_path in enumerate(input_files, 1):
        full_path = os.path.join(curdir, file_path)
        print(f"\n📁 处理文件 {i}/{len(input_files)}: {os.path.basename(file_path)}")
        
        try:
            # 检查文件是否存在
            if not os.path.exists(full_path):
                print(f"   ❌ 文件不存在: {full_path}")
                continue
                
            # 检查文件大小
            file_size = os.path.getsize(full_path)
            print(f"   文件大小: {file_size / (1024*1024):.2f} MB")
            
            # 根据文件大小决定是否使用分批处理
            if file_size > 100 * 1024 * 1024:  # 大于100MB
                print(f"   📦 文件较大，启用分批处理...")
                # 使用分批处理
                cleaned_current = process_file_in_batches(full_path, batch_size=BATCH_SIZE)
            else:
                print(f"   📖 文件较小，使用常规处理...")
                # 常规处理
                current_data = read_jsonl(full_path)
                print(f"   读取记录数: {len(current_data)}")
                
                # 检查数据格式
                if not current_data:
                    print(f"   ⚠️ 文件为空或格式错误")
                    continue
                    
                # 检查第一条记录的字段
                if current_data and len(current_data) > 0:
                    first_record = current_data[0]
                    print(f"   第一条记录字段: {list(first_record.keys())}")
                    
                    # 检查必需字段
                    if 'text' not in first_record:
                        print(f"   ⚠️ 缺少 'text' 字段，尝试使用其他字段...")
                        # 如果没有text字段，尝试使用其他可能的字段
                        if 'content' in first_record:
                            for record in current_data:
                                record['text'] = record.get('content', '')
                        elif 'code' in first_record:
                            for record in current_data:
                                record['text'] = record.get('code', '')
                        else:
                            print(f"   ❌ 无法找到可用的代码字段")
                            continue
                
                # 对当前文件进行预处理和过滤
                print(f"   开始过滤...")
                cleaned_current = data_clean_pipeline(current_data, preprocess_only=True)
                print(f"   过滤后记录数: {len(cleaned_current)}")
                
                # 清理当前文件的内存
                del current_data
            
            # 添加到总列表
            all_cleaned_data.extend(cleaned_current)
            total_processed += len(cleaned_current)
            
            print(f"   ✅ 文件处理完成")
            
        except FileNotFoundError as e:
            print(f"   ❌ 文件不存在: {e}")
            continue
        except json.JSONDecodeError as e:
            print(f"   ❌ JSON格式错误: {e}")
            continue
        except MemoryError as e:
            print(f"   ❌ 内存不足: {e}")
            print(f"   💡 建议: 减少批次大小或使用更小的文件")
            continue
        except Exception as e:
            print(f"   ❌ 处理文件失败: {type(e).__name__}: {e}")
            import traceback
            print(f"   详细错误信息:")
            traceback.print_exc()
            continue
    
    print(f"\n📊 所有文件预处理完成: {len(all_cleaned_data)} 条记录")
    
    if len(all_cleaned_data) == 0:
        print("❌ 没有成功处理任何文件，请检查文件路径和格式")
        sys.exit(1)
    
    # 最后进行去重处理
    print("\n🚀 开始去重...")
    try:
        final_cleaned_data = data_clean_pipeline(all_cleaned_data)
        
        # 保存去重后的数据
        output_file = os.path.join(curdir, "../code_data/cleaned_data/test.jsonl")
        
        # 确保输出目录存在
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        write_jsonl(final_cleaned_data, output_file)
        
        # 输出简化的统计结果
        print(f"\n📈 去重完成: {len(final_cleaned_data)}/{len(all_cleaned_data)} 条记录")
        print(f"总去重率: {((total_processed - len(final_cleaned_data)) / total_processed * 100):.1f}%")
        print(f"输出: {output_file}")
        
    except Exception as e:
        print(f"❌ 去重过程失败: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        
        # 如果去重失败，至少保存预处理后的数据
        backup_file = os.path.join(curdir, "../code_data/cleaned_data/preprocessed_backup.jsonl")
        print(f"💾 保存预处理后的数据到备份文件: {backup_file}")
        os.makedirs(os.path.dirname(backup_file), exist_ok=True)
        write_jsonl(all_cleaned_data, backup_file)
    
    # arkts_data = read_jsonl(os.path.join(curdir, "../../out_rounds/judgements_partially_formatted.jsonl"))
    # for rec in tqdm(arkts_data):
    #     rec["llm_formatted"] = ensure_ets_header(rec["llm_formatted"], rec["path"])
    #     rec["text"] = ensure_ets_header(rec["text"], rec["path"])
    # write_jsonl(arkts_data, os.path.join(curdir, "../../out_rounds/judgements_partially_formatted_with_header.jsonl"))
