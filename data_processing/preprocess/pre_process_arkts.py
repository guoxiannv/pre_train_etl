import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
from utils import *
import re
from typing import Dict, List, Optional
import hashlib
import json
from collections import defaultdict
from datasketch import MinHash, MinHashLSH
import tree_sitter_typescript as tst
from tree_sitter import Language, Parser
from tqdm import tqdm

# --- Part 1: Heuristic Filtering ---

# Configuration for heuristic-based filtering rules.
# Thresholds are based on best practices from projects like The Stack and Codex.
SEED = 42
MAX_AVG_LINE_LENGTH = 100
MAX_LINE_LENGTH = 1000
AUTO_GEN_KEYWORDS = [
    "auto-generated",
    # "autogenerated",
    # "do not edit",
    # "generated by",
]

tree_sitter_language = Language(tst.language_typescript())
parser = Parser()
parser.language = tree_sitter_language


def mask_private_key(text: str) -> str:
    """
    å°† PEM æ ¼å¼çš„ç§é’¥æ®µå†…å®¹æ›¿æ¢æˆ <MASKED_PRIVATE_KEY> å ä½ç¬¦ã€‚
    æ”¯æŒ BEGIN/END PRIVATE KEY ä»¥åŠ BEGIN/END RSA PRIVATE KEYã€‚
    """
    pattern = (
        r'-----BEGIN (?:RSA )?PRIVATE KEY-----'  # å¼€å¤´
        r'.*?'                                   # ä»»æ„å†…å®¹ï¼ˆåŒ…æ‹¬æ¢è¡Œï¼‰
        r'-----END (?:RSA )?PRIVATE KEY-----'    # ç»“æŸ
    )
    return re.sub(
        pattern,
        lambda m: "-----BEGIN PRIVATE KEY-----\n<MASKED_PRIVATE_KEY>\n-----END PRIVATE KEY-----",
        text,
        flags=re.DOTALL | re.IGNORECASE
    )

def auto_generated_filter(text: str) -> bool:
    """Filter out auto-generated files"""
    if not text:
        return False
    lower_content = text.lower()
    return not any(keyword in lower_content for keyword in AUTO_GEN_KEYWORDS)


def avg_line_length_filter(text: str, max_avg_length: int = MAX_AVG_LINE_LENGTH) -> bool:
    """Filter out records with excessive average line length"""
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    total_chars = sum(len(line) for line in lines)
    return (total_chars / len(lines)) <= max_avg_length


def max_line_length_filter(text: str, max_length: int = MAX_LINE_LENGTH) -> bool:
    """Filter out records with extremely long lines"""
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    return not any(len(line) > max_length for line in lines)


# --- Tree-sitter syntax error filter ---

def _count_ts_errors_and_missing(text: str):
    """
    ç»Ÿè®¡ Tree-sitter è§£ææ ‘ä¸­çš„ ERROR èŠ‚ç‚¹æ•°é‡ä¸ missing èŠ‚ç‚¹æ•°é‡ã€‚
    è¿”å›: (error_nodes, missing_nodes, total_nodes)
    """
    if not text or not isinstance(text, str):
        return (0, 0, 0)

    tree = parser.parse(bytes(text, "utf8"))
    root = tree.root_node
    if root is None:
        return (0, 0, 0)

    err_cnt = 0
    miss_cnt = 0
    total = 0

    # DFS éå†
    stack = [root]
    while stack:
        node = stack.pop()
        total += 1

        # æ˜ç¡® ERROR ç±»å‹çš„èŠ‚ç‚¹
        if node.type == "ERROR":
            err_cnt += 1
        # è¯­æ³•ä¸­ç¼ºå¤±çš„å¿…éœ€æ„ä»¶
        if node.is_missing:
            miss_cnt += 1

        # é¿å…é‡å¤è®¡ç®—ï¼šhas_error åªä½œä¸ºå¿«é€Ÿè·¯å¾„ï¼Œä¸å•ç‹¬è®¡æ•°
        # if node.has_error: pass

        # æ¨å…¥å­èŠ‚ç‚¹
        # ç”¨ children è€Œé named_childrenï¼Œç¡®ä¿ä¸æ¼æ‰ç»“æ„æ€§ token
        stack.extend(node.children)

    return (err_cnt, miss_cnt, total)


def tree_sitter_error_filter(
    text: str,
    max_error_nodes: int = 0,
    allow_missing: bool = False,
    max_error_ratio: float = 0.0,
):
    """
    è¯­æ³•é”™è¯¯è¿‡æ»¤å™¨ï¼š
    - è‹¥ ERROR èŠ‚ç‚¹æ•° > max_error_nodesï¼Œåˆ™è¿‡æ»¤ï¼ˆè¿”å› Falseï¼‰
    - è‹¥ä¸å…è®¸ç¼ºå¤±ä¸”å­˜åœ¨ missing èŠ‚ç‚¹ï¼Œåˆ™è¿‡æ»¤
    - è‹¥è®¾ç½®äº† max_error_ratio ä¸” ERROR èŠ‚ç‚¹å æ¯”è¶…è¿‡é˜ˆå€¼ï¼Œä¹Ÿè¿‡æ»¤
    è¿”å› True è¡¨ç¤ºâ€œä¿ç•™â€ï¼ŒFalse è¡¨ç¤ºâ€œè¿‡æ»¤æ‰â€
    """
    err_cnt, miss_cnt, total = _count_ts_errors_and_missing(text)

    # ç¼ºå¤±èŠ‚ç‚¹æ§åˆ¶
    if not allow_missing and miss_cnt > 0:
        return False

    # ç»å¯¹æ•°é‡é˜ˆå€¼
    if err_cnt > max_error_nodes:
        return False

    # å æ¯”é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
    if total > 0 and max_error_ratio > 0.0:
        if (err_cnt / total) > max_error_ratio:
            return False

    return True


def _normalize_for_syntax(text: str) -> str:
    """
    ä»…ç”¨äºè¯­æ³•æ£€æŸ¥çš„æ ‡å‡†åŒ–ï¼š
    - å¤„ç†å¸¸è§è½¬ä¹‰åºåˆ—ï¼ˆ\\n/\\t/\\r/\"/\'/\\ï¼‰
    - ç»Ÿä¸€æ¢è¡Œç¬¦ä¸º \n
    ä¸æ”¹å˜åŸå§‹æ–‡æœ¬ï¼Œåªè¿”å›ç”¨äº parse çš„å‰¯æœ¬ã€‚
    """
    if not text or not isinstance(text, str):
        return ""

    normalized = text
    # å¸¸è§è½¬ä¹‰åºåˆ—
    escape_mappings = {
        r"\\n": "\n",
        r"\\t": "\t",
        r"\\r": "\r",
        r"\\\"": '"',
        r"\\'": "'",
        r"\\\\": "\\",
    }
    for escaped, unescaped in escape_mappings.items():
        normalized = normalized.replace(escaped, unescaped)

    # å¤„ç†å…¶ä»–å½¢å¼çš„å•å­—ç¬¦è½¬ä¹‰
    normalized = re.sub(r"\\(.)", r"\1", normalized)

    # ç»Ÿä¸€æ¢è¡Œ
    normalized = normalized.replace("\r\n", "\n").replace("\r", "\n")

    return normalized


def arkts_syntax_filter(text: str, max_errors: int = 0, allow_missing: bool = False):
    """
    ä¸¥æ ¼è¯­æ³•æ ¡éªŒï¼ˆå‚è€ƒ syntax_check_filter.pyï¼‰ï¼š
    - åªè¦å­˜åœ¨ä»»ä½• ERROR èŠ‚ç‚¹æˆ– missing èŠ‚ç‚¹ï¼ˆå½“ allow_missing=Falseï¼‰å³åˆ¤ä¸ºå¤±è´¥
    - è¿”å› (passed: bool, err_cnt: int, miss_cnt: int, total_nodes: int)
    """
    if not text or not isinstance(text, str):
        return False, 0, 0, 0

    norm = _normalize_for_syntax(text)
    err_cnt, miss_cnt, total_nodes = _count_ts_errors_and_missing(norm)

    if not allow_missing and miss_cnt > 0:
        return False, err_cnt, miss_cnt, total_nodes
    if err_cnt > max_errors:
        return False, err_cnt, miss_cnt, total_nodes
    return True, err_cnt, miss_cnt, total_nodes

def empty_content_filter(text: str) -> bool:
    """Filter out records with no content"""
    if not text:
        return False
    lines = text.splitlines()
    return bool(lines)


# --- Part 2: Exact Deduplication ---

def get_content_hash(content):
    """
    Computes the SHA256 hash of a string content.

    Args:
        content (str): The string content to hash.

    Returns:
        str: The hexadecimal SHA256 hash digest.
    """
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def find_exact_duplicates(records):
    """
    Finds records with exactly duplicate content using hashing.

    Args:
        records (list[dict]): A list of records to check for duplicates.
                               Each record must have 'id' and 'content'.

    Returns:
        set: A set of 'id's of the records to be removed.
    """
    hashes = defaultdict(list)
    for record in records:
        content_hash = get_content_hash(record['text'])
        hashes[content_hash].append(record['id'])

    to_remove = set()
    for hash_val, ids in hashes.items():
        if len(ids) > 1:
            # Keep the first one, mark the rest for removal
            to_remove.update(ids[1:])
            print(f"  - Found {len(ids)} exact duplicates. Keeping '{ids}', removing {ids[1:]}.")

    return to_remove


# --- Part 3: Near-Deduplication ---

# Configuration for MinHash and LSH parameters.
# Based on industry standards and recommendations from The Stack.
JACCARD_THRESHOLD = 0.85
NUM_PERMUTATIONS = 256


def find_near_duplicates(records):
    """
    Finds near-duplicate records using MinHash and LSH.

    Args:
        records (list[dict]): A list of records to check.

    Returns:
        set: A set of 'id's of the near-duplicate records to be removed.
    """
    print("\n Creating MinHash signatures...")

    # 1. Create a MinHash signature for each record
    minhashes = {}
    for i, record in enumerate(records):
        content = record.get("text", "")
        if not content:
            continue

        # Create shingles (5-grams of characters) from the content
        shingles = {content[i:i + 5] for i in range(len(content) - 4)}
        if not shingles:
            continue

        m = MinHash(num_perm=NUM_PERMUTATIONS, seed=SEED)
        for s in shingles:
            m.update(s.encode('utf8'))
        minhashes[record['id']] = m

        if (i + 1) % 200 == 0:
            print(f"  - Processed {i + 1}/{len(records)} records...")

    print(f" MinHash signatures created for {len(minhashes)} records.")
    print(" Building LSH index...")

    # 2. Index all MinHash signatures in the LSH model
    lsh = MinHashLSH(threshold=JACCARD_THRESHOLD, num_perm=NUM_PERMUTATIONS)
    for record_id, m in minhashes.items():
        lsh.insert(record_id, m)

    print(" LSH index built. Querying for duplicate clusters...")

    # 3. Query for duplicates and identify clusters
    processed_ids = set()
    clusters = []
    for record_id, m in minhashes.items():
        if record_id in processed_ids:
            continue

        # Find candidate pairs for the current record
        result = lsh.query(m)

        if len(result) > 1:
            cluster = set(result)
            clusters.append(list(cluster))
            processed_ids.update(cluster)

    # 4. From each cluster, keep one record and mark others for removal
    to_remove = set()
    for cluster in clusters:
        # A simple strategy: keep the record with the lexicographically smallest ID
        cluster.sort()
        kept_id = cluster[0]
        removed_ids = cluster[1:]
        to_remove.update(removed_ids)

        print(f"  - Cluster found. Keeping '{kept_id}', removing {removed_ids}")

    return to_remove

def remove_angle_bracket_headers(text: str) -> str:
    """
    Remove all lines containing <any content> format from the beginning of the code.
    Only processes consecutive lines containing angle bracket tags at the start of the file.
    
    Args:
        text: String containing the code
        
    Returns:
        Code string after removing all <any content> header information
    """
    lines = text.split('\n')
    # Find index of first line not containing <tag> format
    start_index = 0
    for i, line in enumerate(lines):
        # Check if line contains <tag> format (excluding cases within quotes)
        # Simple heuristic: if line starts with < or main content is <tag>, consider it header info
        stripped_line = line.strip()
        if not stripped_line:
            continue
        # If line contains <tag> and looks like header info (no common code syntax)
        if ('<' in line and '>' in line and 
            not any(keyword in line for keyword in ['=', '(', ')', '{', '}', ';', 'function', 'const', 'let', 'var', 'class', 'import', 'export']) and
            not line.strip().startswith('//')): # Not a comment
            start_index = i + 1
        else:
            break
    
    return '\n'.join(lines[start_index:])


def clean_code_headers(code: str) -> str:
    """
    Uses regular expressions to remove Copyright and Licensed comment blocks from the code.

    Args:
        code: A string containing the code.

    Returns:
        The code string after removing the copyright and license statements.
    """
    # This regular expression finds multi-line comment blocks (/* ... */)
    # containing the keywords "Copyright" or "Licensed"
    # The re.DOTALL flag allows '.' to match any character, including newlines.
    # The re.IGNORECASE flag makes the matching case-insensitive.
    # \s* will match and remove trailing whitespace (like newlines) after the comment block for cleaner formatting.
    pattern = r'/\*+.*?(?:Copyright|Licensed).*?\*/\s*'

    cleaned_code = re.sub(pattern, '', code, flags=re.DOTALL | re.IGNORECASE)

    return cleaned_code.lstrip()  # Remove potential leading blank lines created by deleting the comment.

def remove_single_line_comments(text: str) -> str:
    """Strip // comments"""
    return re.sub(r'//.*', '', text)

def remove_leading_blank_lines(text: str) -> str:
    """
    Remove blank lines (empty or whitespace-only) at the very beginning of the text.
    """
    if not isinstance(text, str) or not text:
        return text
    lines = text.split('\n')
    start_index = 0
    while start_index < len(lines) and lines[start_index].strip() == '':
        start_index += 1
    return '\n'.join(lines[start_index:])

def ensure_ets_header(text: str, path: Optional[str]) -> str:
    """
    Ensure the first non-empty line is a header like "// dir/file.ets".
    If missing and `path` is provided, prepend a header built from the last two segments of `path`.
    The file name is normalized to have a .ets extension.
    """
    if not isinstance(text, str):
        return text

    header_regex = re.compile(r'^\s*//\s*.+\.ets\s*$', re.IGNORECASE)
    lines = text.split('\n')

    # Find first non-empty line
    first_idx = 0
    while first_idx < len(lines) and lines[first_idx].strip() == '':
        first_idx += 1

    if first_idx < len(lines) and header_regex.match(lines[first_idx] or ''):
        return text

    if not path:
        return text

    norm_path = os.path.normpath(path)
    parts = [p for p in norm_path.split(os.sep) if p]
    if not parts:
        return text
    last_two = '/'.join([parts[-1]]) if len(parts) >= 2 else parts[-1]
    header_line = f"// {last_two}"
    return header_line + ('\n' if text and not text.startswith('\n') else '\n') + text

# --- New Filter Functions ---

def min_line_count_filter(text: str, min_lines: int = 5) -> bool:
    """Keep if text has at least min_lines non-empty lines"""
    content_lines = get_content_lines(text)
    return len(content_lines) >= min_lines


def blank_vs_content_filter(text: str, max_ratio: float = 0.8, min_blank: int = 14) -> bool:
    """Filter out if blank lines exceed content lines or ratio threshold"""
    lines = text.splitlines()
    content_lines = [l for l in lines if l.strip()]
    blank = len(lines) - len(content_lines)
    if blank > len(content_lines):
        return False
    if len(content_lines) > 0 and (blank / len(content_lines) > max_ratio and blank >= min_blank):
        return False
    return True


def colon_density_filter(text: str, threshold: float = 0.45, min_content: int = 80) -> bool:
    """Filter out ArkTS-heavy records with high colon density"""
    content_lines = [l for l in text.splitlines() if l.strip()]
    if len(content_lines) < min_content:
        return True
    density = sum(1 for l in content_lines if ':' in l) / len(content_lines)
    return density <= threshold


def trivial_assignment_filter(text: str, vars: List[str] = ['a','b','c']) -> bool:
    """Drop if trivial assignments like 'a =', 'b=' etc. appear"""
    pattern = rf"\b({'|'.join(vars)})\s*="
    return re.search(pattern, text) is None


def get_content_lines(text: str) -> List[str]:
    """Extract non-empty, non-comment lines from text"""
    return [line.strip() for line in text.splitlines() 
            if line.strip() and not line.strip().startswith('//')]

def prefix_repetition_filter(text: str, prefix_len: int = 16, max_density: float = 0.4) -> bool:
    """Ensure no prefix of length prefix_len repeats more than max_density ratio of content lines"""
    # Get non-empty, non-comment lines
    content_lines = get_content_lines(text)
    if not content_lines:
        return False
        
    # Count prefix occurrences
    counts = defaultdict(int)
    for line in content_lines:
        if len(line) <= prefix_len:
            continue
        prefix = line[:prefix_len]
        counts[prefix] += 1
    
    # Check if any prefix density exceeds threshold
    total_lines = len(content_lines)
    for count in counts.values():
        if count / total_lines > max_density:
            return False
            
    return True


def chinese_char_threshold_filter(text: str, threshold: float = 0.3) -> bool:
    """Drop if Chinese character ratio exceeds threshold"""
    # Get content lines to calculate valid characters
    content_lines = get_content_lines(text)
    if not content_lines:
        return True
    
    # Calculate total valid characters (excluding whitespace)
    total_chars = sum(len(line.replace(' ', '').replace('\t', '')) for line in content_lines)
    if total_chars == 0:
        return True
    
    # Count Chinese characters
    chinese_count = sum(1 for c in ''.join(content_lines) if '\u4e00' <= c <= '\u9fff')
    chinese_ratio = chinese_count / total_chars
    
    return chinese_ratio <= threshold


def forbidden_token_frequency_filter(text: str, limits: Optional[Dict[str, int]] = None) -> bool:
    """Drop if any forbidden token appears more than allowed times"""
    if limits is None:
        limits = {'public': 6, 'private': 6}
    return not any(text.count(tok) > thr for tok, thr in limits.items())

def numeric_literal_density_filter(text: str, max_density: float = 0.3) -> bool:
    """
    å¦‚æœæ•°å€¼å¸¸é‡åœ¨æ‰€æœ‰ token ä¸­å æ¯”è¶…è¿‡ max_densityï¼Œåˆ™è¿‡æ»¤æ‰è¯¥ recordã€‚
    """
    # åŒ¹é…æ‰€æœ‰æ•°å€¼å¸¸é‡ï¼šæ•´æ•°æˆ–æµ®ç‚¹æ•°
    num_literals = re.findall(r'\b-?\d+(\.\d+)?\b', text)
    # åŒ¹é…æ‰€æœ‰å•è¯ tokenï¼ˆåŒ…æ‹¬æ ‡è¯†ç¬¦ã€å…³é”®å­—ã€å¸¸é‡ç­‰ï¼‰
    all_tokens = re.findall(r'\b\w+\b', text)
    if not all_tokens:
        return False
    density = len(num_literals) / len(all_tokens)
    return density <= max_density

def numeric_array_length_filter(text: str, max_len: int = 10) -> bool:
    """
    æ£€æµ‹ä»£ç ä¸­æ‰€æœ‰ [...] æ•°ç»„åˆå§‹åŒ–ï¼š
    è‹¥æ•°ç»„å†…æ‰€æœ‰å…ƒç´ å‡ä¸ºæ•°å€¼ä¸”å…ƒç´ æ•°é‡ > max_lenï¼Œåˆ™è¿‡æ»¤æ‰è¯¥ recordã€‚
    """
    for match in re.finditer(r'\[([^\]]+)\]', text):
        elems = [e.strip() for e in match.group(1).split(',') if e.strip()]
        # åˆ¤æ–­æ˜¯å¦å…¨éƒ¨ä¸ºæ•°å­—å­—é¢é‡
        if elems and all(re.fullmatch(r'-?\d+(\.\d+)?', e) for e in elems):
            if len(elems) > max_len:
                return False
    return True

def total_length_filter(text: str, max_length: int = 30696) -> bool:
    """Filter out records with total character count exceeding max_length"""
    return len(text) <= max_length


def count_empty_functions(text: str):
    """
    è¿”å› (total_functions, empty_functions)
    ç©ºå‡½æ•°å®šä¹‰ä¸ºï¼šå‡½æ•°ä½“é‡Œåªæœ‰å¤§æ‹¬å·ï¼Œæ²¡æœ‰ä»»ä½•å­èŠ‚ç‚¹è¯­å¥ã€‚
    """
    tree = parser.parse(bytes(text, 'utf8'))
    root = tree.root_node

    if root is None:
        return 0, 0

    total, empty = 0, 0
    # Tree-sitter ä¸­å¸¸è§çš„å‡½æ•°/æ–¹æ³•èŠ‚ç‚¹ç±»å‹ï¼Œè§†è¯­è¨€è€Œå®šå¯å¢åˆ 
    fn_types = {
        'function_declaration',
        'method_definition',
        'function',
        'arrow_function',
        'function_signature'  # æ ¹æ®ä½ ç”¨çš„ TypeScript è¯­æ³•æ ‘å¯èƒ½è¦è°ƒæ•´
    }

    # ä½¿ç”¨æ˜¾å¼æ ˆè¿›è¡Œ DFSï¼Œé¿å… cursor.node çš„å¯ç©ºç±»å‹å‘Šè­¦
    stack = [root]
    while stack:
        node = stack.pop()
        if getattr(node, 'type', None) in fn_types:
            total += 1
            # æ‰¾åˆ°ç›´æ¥å­èŠ‚ç‚¹é‡Œçš„ `{ ... }`
            for c in getattr(node, 'children', []):
                if getattr(c, 'type', None) == 'statement_block':
                    non_brace_children = [
                        gc for gc in getattr(c, 'children', [])
                        if getattr(gc, 'type', None) not in ('{', '}')
                    ]
                    if len(non_brace_children) == 0:
                        empty += 1
                    break
        # æ¨å…¥å­èŠ‚ç‚¹
        stack.extend(getattr(node, 'children', []))

    return total, empty


def empty_function_density_filter(text: str,
                                  max_empty: int = 5,
                                  max_ratio: float = 0.3) -> bool:
    """
    ç©ºå‡½æ•°æ•° > max_empty æˆ– ç©ºå‡½æ•°/æ€»å‡½æ•° > max_ratio æ—¶ï¼Œè¿”å› Falseï¼ˆå³è¿‡æ»¤æ‰ï¼‰ã€‚
    """
    total, empty = count_empty_functions(text)
    if total == 0:
        return True  # æ²¡æœ‰å‡½æ•°ï¼Œä¸èµ°è¿™æ¡è§„åˆ™
    if empty > max_empty or (empty / total) > max_ratio:
        return False
    return True

# é¢„ç¼–è¯‘è‹¥å¹²å¸¸è§æ¨¡æ¿è¯­æ³•çš„æ­£åˆ™
_RE_JINJA_STMT = re.compile(r"{%-?\s*[^%]+?-?%}")                 # {% ... %}
_RE_JINJA_EXPR = re.compile(r"{{-?\s*[^}]+?-?}}")                 # {{ ... }}
_RE_JINJA_CTRL = re.compile(r"{%-?\s*(for|if|elif|else|endif|endfor|block|endblock|extends|include|set|macro|endmacro)\b[^%]*-?%}")

_RE_MUSTACHE_SEC = re.compile(r"{{\s*[#/\^>]\s*[^}]+}}")          # {{#section}}, {{/section}}, {{^inverse}}, {{> partial}}
_RE_HANDLEBARS_TRIPLE = re.compile(r"{{{[^}]+}}}")                # {{{ ... }}}

_RE_EJS_ERB = re.compile(r"<%[\s\S]*?%>")                         # EJS/ERB <% ... %>

# ä½ ç¤ºä¾‹ä¸­çš„â€œ/*--- ... ---*/â€å—ï¼ˆYAMLé£æ³¨é‡Šï¼‰ä¹Ÿä½œä¸ºä¿¡å·
_RE_YAML_BLOCK_COMMENT = re.compile(r"/\*\s*---[\s\S]+?---\s*\*/")

def jinja_template_filter(code: str) -> bool:
    """
    æ£€æµ‹ä»£ç ç‰‡æ®µæ˜¯å¦åŒ…å«å¸¸è§æ¨¡æ¿è¯­æ³•ï¼ˆJinja/Nunjucks/Mustache/Handlebars/EJS/ERB ç­‰ï¼‰ã€‚
    è¿”å› True è¡¨ç¤ºâ€œåº”è¿‡æ»¤ï¼ˆç–‘ä¼¼æ¨¡æ¿ï¼‰â€ï¼›False è¡¨ç¤ºâ€œä¿ç•™â€ã€‚
    """
    if not code or not isinstance(code, str):
        return False

    # é«˜ç½®ä¿¡åº¦å‘½ä¸­ï¼šä»»ä½•ä¸€ç§å³å¯ç›´æ¥è¿‡æ»¤
    if (_RE_JINJA_STMT.search(code) or
        _RE_JINJA_CTRL.search(code) or
        _RE_EJS_ERB.search(code) or
        _RE_MUSTACHE_SEC.search(code) or
        _RE_HANDLEBARS_TRIPLE.search(code) or
        _RE_YAML_BLOCK_COMMENT.search(code)):
        return True

    # ä¸­ç­‰ç½®ä¿¡åº¦ï¼šå¤§é‡ {{ ... }} è¡¨è¾¾å¼ï¼ˆä½†é¿å…æŠŠæ™®é€šè¯­è¨€è¯¯æ€ï¼‰
    # ArkTS/TS/JS æ­£å¸¸ä¸ä¼šåœ¨ä»£ç é‡Œä½¿ç”¨ {{}}ï¼ˆæ¨¡æ¿å­—ç¬¦ä¸²æ˜¯ ${}ï¼‰ï¼Œ
    # è‹¥å‡ºç°å¤šå¤„ {{...}}ï¼Œå¯è®¤ä¸ºæ˜¯æ¨¡æ¿ã€‚
    exprs = _RE_JINJA_EXPR.findall(code)
    if len(exprs) >= 2:
        return True

    # ä½ç½®ä¿¡åº¦è¡¥å……ï¼šåŒæ—¶å‡ºç° {% å’Œ %} ä¹Ÿå¾ˆå¯ç–‘
    if code.count("{%") >= 1 and code.count("%}") >= 1:
        return True

    return False


def data_clean_pipeline(corpus: List[Dict], out_dir: str = "./filter_logs", preprocess_only: bool = False):
    """
    Cleans the input corpus by removing comments, filtering out empty texts,
    and ensuring that the text does not contain more than 5 Chinese characters.

    Args:
        corpus: A list of dictionaries, each containing a 'text' key with the code to be cleaned.
        out_dir: Directory to save filter logs.
        preprocess_only: If True, only run preprocessing steps (mask private keys, remove headers, clean code headers)
                        without applying filters and deduplication.

    Returns:
        A list of cleaned dictionaries.
    """
    os.makedirs(out_dir, exist_ok=True)
    removed = defaultdict(list)
    syntax_error_details = []  # æ”¶é›†è¯­æ³•é”™è¯¯çš„è¯¦ç»†ä¿¡æ¯
    print("--- Step 0: Begin with {} records ---".format(len(corpus)))
    tmp_corpus = []

    ## Preprocess and normalize headers
    for item in corpus:
        text = item.get("text", "")
        # 1) Mask PrivateKey
        text = mask_private_key(text)
        # 2) Trim leading blank lines
        text = remove_leading_blank_lines(text)
        # 3) Remove angle bracket headers at the very beginning
        text = remove_angle_bracket_headers(text)
        # 4) Remove license/copyright blocks
        text = clean_code_headers(text)
        # 5) Trim leading blank lines again in case previous step introduced them
        text = remove_leading_blank_lines(text)
        # 6) Ensure // path/to/file.ets header exists using last two segments of path
        text = ensure_ets_header(text, item.get("path"))

        item['text'] = text
        tmp_corpus.append(item)

    # If preprocess_only is True, skip all filtering and deduplication steps
    if preprocess_only:
        print("--- Preprocessing Only Mode: Skipping filters and deduplication ---")
        print(f"Returning {len(tmp_corpus)} preprocessed records.")
        return tmp_corpus

    print("--- Step 1: All Filters ---")
    aft = []
    for r in tmp_corpus:
        t = r['text']
        
        # Heuristic filters (æ‹†åˆ†åçš„ç‹¬ç«‹è¿‡æ»¤å™¨)
        if not empty_content_filter(t):
            removed['empty_content_filter'].append(r)
            continue
        if not auto_generated_filter(t):
            removed['auto_generated_filter'].append(r)
            continue
        if not avg_line_length_filter(t):
            removed['avg_line_length_filter'].append(r)
            continue
        if not max_line_length_filter(t):
            removed['max_line_length_filter'].append(r)
            continue
        # Additional filters
        # Tree-sitter ä¸¥æ ¼è¯­æ³•è¿‡æ»¤ï¼šåªè¦å­˜åœ¨ä»»ä½• ERROR æˆ– missingï¼ˆä¸å…è®¸ï¼‰å³è¿‡æ»¤
        passed, err_cnt, miss_cnt, total_nodes = arkts_syntax_filter(t)
        if not passed:
            removed['tree_sitter_error_filter'].append(r)
            syntax_error_details.append({
                'id': r.get('id'),
                'corpusid': r.get('corpusid', ''),
                'error_count': err_cnt,
                'missing_count': miss_cnt,
                'total_nodes': total_nodes,
                'error_type': 'è¯­æ³•é”™è¯¯',
                'text_preview': (t[:200] + '...') if isinstance(t, str) and len(t) > 200 else t
            })
            continue
        if not min_line_count_filter(t):
            removed['min_line_count_filter'].append(r)
            continue
        if not blank_vs_content_filter(t):
            removed['blank_vs_content_filter'].append(r)
            continue
        if not colon_density_filter(t):
            removed['colon_density_filter'].append(r)
            continue
        if not trivial_assignment_filter(t):
            removed['trivial_assignment_filter'].append(r)
            continue
        if not prefix_repetition_filter(t, prefix_len=16):
            removed['prefix_repetition_filter'].append(r)
            continue
        if not chinese_char_threshold_filter(t):
            removed['chinese_char_threshold_filter'].append(r)
            continue
        if not forbidden_token_frequency_filter(t):
            removed['forbidden_token_frequency_filter'].append(r)
            continue
        if not numeric_literal_density_filter(t):
            removed['numeric_literal_density_filter'].append(r)
            continue
        if not numeric_array_length_filter(t):
            removed['numeric_array_length_filter'].append(r)
            continue
        if not total_length_filter(t):
            removed['total_length_filter'].append(r)
            continue
        if not empty_function_density_filter(t):
            removed['empty_function_density_filter'].append(r)
            continue
        # if not jinja_template_filter(t):
        #     removed['jinja_template_filter'].append(r)
        #     continue
        aft.append(r)

    # åˆ†åˆ«å†™å…¥æ¯ä¸ª filter ä¸¢å¼ƒè®°å½•
    for name, recs in {
        "removed_empty_content_filter": removed['empty_content_filter'],
        "removed_auto_generated_filter": removed['auto_generated_filter'],
        "removed_avg_line_length_filter": removed['avg_line_length_filter'],
        "removed_max_line_length_filter": removed['max_line_length_filter'],
        "removed_min_line_count_filter": removed['min_line_count_filter'],
        "removed_blank_vs_content_filter": removed['blank_vs_content_filter'],
        "removed_colon_density_filter": removed['colon_density_filter'],
        "removed_trivial_assignment_filter": removed['trivial_assignment_filter'],
        "removed_prefix_repetition_filter": removed['prefix_repetition_filter'],
        "removed_chinese_char_threshold_filter": removed['chinese_char_threshold_filter'],
        "removed_forbidden_token_frequency_filter": removed['forbidden_token_frequency_filter'],
        "removed_numeric_literal_density_filter": removed['numeric_literal_density_filter'],
        "removed_numeric_array_length_filter": removed['numeric_array_length_filter'],
        "removed_total_length_filter": removed['total_length_filter'],
        "removed_empty_function_density_filter": removed['empty_function_density_filter'],
        # "removed_jinja_template_filter": removed['jinja_template_filter'],
        # "removed_tree_sitter_error_filter": removed['tree_sitter_error_filter'],
    }.items():
        write_jsonl(recs, os.path.join(out_dir, f"{name}.jsonl"))
        print(f"{name} ä¸¢å¼ƒ {len(recs)} æ¡")

    # é¢å¤–è¾“å‡ºè¯­æ³•é”™è¯¯è¯¦æƒ…æŠ¥å‘Š
    if syntax_error_details:
        details_path = os.path.join(out_dir, "removed_tree_sitter_error_details.jsonl")
        write_jsonl(syntax_error_details, details_path)
        print(f"removed_tree_sitter_error_details è®°å½• {len(syntax_error_details)} æ¡ -> {details_path}")

    print(f"Kept {len(aft)} after all filters.\n")

    # Step 2: Exact Deduplication
    print("--- Step 2: Applying Exact Deduplication ---")
    exact_dup_ids = find_exact_duplicates(aft)
    records_after_exact_dedup = [r for r in aft if r['id'] not in exact_dup_ids]
    print(f"Exact deduplication complete. Kept {len(records_after_exact_dedup)} records.\n")

    # Step 3: Near-Deduplication
    print("--- Step 3: Applying Near-Deduplication ---")
    near_dup_ids = find_near_duplicates(records_after_exact_dedup)
    final_clean_records = [r for r in records_after_exact_dedup if r['id'] not in near_dup_ids]
    print(f"Near-deduplication complete. Kept {len(final_clean_records)} records.\n")

    return final_clean_records



def process_file_in_batches(file_path: str, batch_size: int = 500):
    """
    åˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤500æ¡è®°å½•
        
    Returns:
        å¤„ç†åçš„è®°å½•åˆ—è¡¨
    """
    processed_records = []
    total_lines = 0
    processed_lines = 0
    
    print(f"   å¼€å§‹åˆ†æ‰¹å¤„ç†ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            batch = []
            batch_count = 0
            
            for line_num, line in enumerate(f, 1):
                total_lines += 1
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    record = json.loads(line)
                    batch.append(record)
                    processed_lines += 1
                    
                    # å½“æ‰¹æ¬¡æ»¡äº†ï¼Œå¤„ç†è¿™ä¸€æ‰¹
                    if len(batch) >= batch_size:
                        batch_count += 1
                        print(f"   å¤„ç†æ‰¹æ¬¡ {batch_count}: {len(batch)} æ¡è®°å½•")
                        
                        # å¤„ç†å½“å‰æ‰¹æ¬¡
                        cleaned_batch = data_clean_pipeline(batch, preprocess_only=True)
                        processed_records.extend(cleaned_batch)
                        
                        print(f"   æ‰¹æ¬¡ {batch_count} å®Œæˆ: {len(batch)} -> {len(cleaned_batch)} æ¡")
                        
                        # æ¸…ç†æ‰¹æ¬¡å†…å­˜
                        del batch
                        batch = []
                        
                        # æ¯å¤„ç†å‡ ä¸ªæ‰¹æ¬¡åæ˜¾ç¤ºè¿›åº¦
                        if batch_count % 5 == 0:
                            print(f"   ğŸ“Š å·²å¤„ç† {batch_count} ä¸ªæ‰¹æ¬¡ï¼Œç´¯è®¡è®°å½•: {len(processed_records)}")
                        
                except json.JSONDecodeError as e:
                    if line_num <= 10:  # åªåœ¨å‰10è¡Œæ˜¾ç¤ºé”™è¯¯
                        print(f"   è­¦å‘Šï¼šç¬¬ {line_num} è¡ŒJSONæ ¼å¼é”™è¯¯: {e}")
                    continue
                except Exception as e:
                    if line_num <= 10:
                        print(f"   è­¦å‘Šï¼šç¬¬ {line_num} è¡Œå¤„ç†é”™è¯¯: {e}")
                    continue
        
        # å¤„ç†æœ€åä¸€æ‰¹
        if batch:
            batch_count += 1
            print(f"   å¤„ç†æœ€åæ‰¹æ¬¡ {batch_count}: {len(batch)} æ¡è®°å½•")
            cleaned_batch = data_clean_pipeline(batch, preprocess_only=True)
            processed_records.extend(cleaned_batch)
            print(f"   æœ€åæ‰¹æ¬¡å®Œæˆ: {len(batch)} -> {len(cleaned_batch)} æ¡")
            del batch
        
        print(f"   ğŸ“Š åˆ†æ‰¹å¤„ç†å®Œæˆ: æ€»è¡Œæ•° {total_lines}, æœ‰æ•ˆè®°å½• {processed_lines}, å¤„ç†å {len(processed_records)} æ¡")
        
    except Exception as e:
        print(f"   âŒ åˆ†æ‰¹å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    return processed_records


# --- Test Cases ---
if __name__ == "__main__":
    # Get the current working directory where the script is run from
    curdir = os.path.dirname(__file__)
    print(f"Running from directory: {curdir}")
    
    # å®šä¹‰è¾“å…¥æ–‡ä»¶åˆ—è¡¨
    input_files = [
        "../code_data/raw_data/harmony_samples_formated.jsonl",
        "../code_data/cleaned_data/arkui_2k_pretrain_cleaned_formated.jsonl", 
        "../docs_data/pure_code.jsonl",
        "../code_data/raw_data/dz5484.jsonl"
    ]
    
    # å¯é…ç½®çš„æ‰¹æ¬¡å¤§å°ï¼Œæ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´
    BATCH_SIZE = 500  # å¯ä»¥æ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´ï¼šå†…å­˜å°ç”¨200-300ï¼Œå†…å­˜å¤§ç”¨500-1000
    
    print(f"ğŸ’¾ æ‰¹æ¬¡å¤§å°é…ç½®: {BATCH_SIZE} æ¡è®°å½•/æ‰¹æ¬¡")
    print(f"ğŸ’¡ å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å‡å°è¿™ä¸ªå€¼")
    
    # é€ä¸ªæ–‡ä»¶å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
    all_cleaned_data = []
    total_processed = 0
    
    for i, file_path in enumerate(input_files, 1):
        full_path = os.path.join(curdir, file_path)
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i}/{len(input_files)}: {os.path.basename(file_path)}")
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(full_path):
                print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
                continue
                
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(full_path)
            print(f"   æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")
            
            # æ ¹æ®æ–‡ä»¶å¤§å°å†³å®šæ˜¯å¦ä½¿ç”¨åˆ†æ‰¹å¤„ç†
            if file_size > 100 * 1024 * 1024:  # å¤§äº100MB
                print(f"   ğŸ“¦ æ–‡ä»¶è¾ƒå¤§ï¼Œå¯ç”¨åˆ†æ‰¹å¤„ç†...")
                # ä½¿ç”¨åˆ†æ‰¹å¤„ç†
                cleaned_current = process_file_in_batches(full_path, batch_size=BATCH_SIZE)
            else:
                print(f"   ğŸ“– æ–‡ä»¶è¾ƒå°ï¼Œä½¿ç”¨å¸¸è§„å¤„ç†...")
                # å¸¸è§„å¤„ç†
                current_data = read_jsonl(full_path)
                print(f"   è¯»å–è®°å½•æ•°: {len(current_data)}")
                
                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if not current_data:
                    print(f"   âš ï¸ æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                    continue
                    
                # æ£€æŸ¥ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µ
                if current_data and len(current_data) > 0:
                    first_record = current_data[0]
                    print(f"   ç¬¬ä¸€æ¡è®°å½•å­—æ®µ: {list(first_record.keys())}")
                    
                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    if 'text' not in first_record:
                        print(f"   âš ï¸ ç¼ºå°‘ 'text' å­—æ®µï¼Œå°è¯•ä½¿ç”¨å…¶ä»–å­—æ®µ...")
                        # å¦‚æœæ²¡æœ‰textå­—æ®µï¼Œå°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„å­—æ®µ
                        if 'content' in first_record:
                            for record in current_data:
                                record['text'] = record.get('content', '')
                        elif 'code' in first_record:
                            for record in current_data:
                                record['text'] = record.get('code', '')
                        else:
                            print(f"   âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨çš„ä»£ç å­—æ®µ")
                            continue
                
                # å¯¹å½“å‰æ–‡ä»¶è¿›è¡Œé¢„å¤„ç†å’Œè¿‡æ»¤
                print(f"   å¼€å§‹è¿‡æ»¤...")
                cleaned_current = data_clean_pipeline(current_data, preprocess_only=True)
                print(f"   è¿‡æ»¤åè®°å½•æ•°: {len(cleaned_current)}")
                
                # æ¸…ç†å½“å‰æ–‡ä»¶çš„å†…å­˜
                del current_data
            
            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_cleaned_data.extend(cleaned_current)
            total_processed += len(cleaned_current)
            
            print(f"   âœ… æ–‡ä»¶å¤„ç†å®Œæˆ")
            
        except FileNotFoundError as e:
            print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {e}")
            continue
        except json.JSONDecodeError as e:
            print(f"   âŒ JSONæ ¼å¼é”™è¯¯: {e}")
            continue
        except MemoryError as e:
            print(f"   âŒ å†…å­˜ä¸è¶³: {e}")
            print(f"   ğŸ’¡ å»ºè®®: å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ›´å°çš„æ–‡ä»¶")
            continue
        except Exception as e:
            print(f"   âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            print(f"   è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            continue
    
    print(f"\nğŸ“Š æ‰€æœ‰æ–‡ä»¶é¢„å¤„ç†å®Œæˆ: {len(all_cleaned_data)} æ¡è®°å½•")
    
    if len(all_cleaned_data) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼")
        sys.exit(1)
    
    # æœ€åè¿›è¡Œå»é‡å¤„ç†
    print("\nğŸš€ å¼€å§‹å»é‡...")
    try:
        final_cleaned_data = data_clean_pipeline(all_cleaned_data)
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        output_file = os.path.join(curdir, "../code_data/cleaned_data/test.jsonl")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        write_jsonl(final_cleaned_data, output_file)
        
        # è¾“å‡ºç®€åŒ–çš„ç»Ÿè®¡ç»“æœ
        print(f"\nğŸ“ˆ å»é‡å®Œæˆ: {len(final_cleaned_data)}/{len(all_cleaned_data)} æ¡è®°å½•")
        print(f"æ€»å»é‡ç‡: {((total_processed - len(final_cleaned_data)) / total_processed * 100):.1f}%")
        print(f"è¾“å‡º: {output_file}")
        
    except Exception as e:
        print(f"âŒ å»é‡è¿‡ç¨‹å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        
        # å¦‚æœå»é‡å¤±è´¥ï¼Œè‡³å°‘ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
        backup_file = os.path.join(curdir, "../code_data/cleaned_data/preprocessed_backup.jsonl")
        print(f"ğŸ’¾ ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°å¤‡ä»½æ–‡ä»¶: {backup_file}")
        os.makedirs(os.path.dirname(backup_file), exist_ok=True)
        write_jsonl(all_cleaned_data, backup_file)
    
    # arkts_data = read_jsonl(os.path.join(curdir, "../../out_rounds/judgements_partially_formatted.jsonl"))
    # for rec in tqdm(arkts_data):
    #     rec["llm_formatted"] = ensure_ets_header(rec["llm_formatted"], rec["path"])
    #     rec["text"] = ensure_ets_header(rec["text"], rec["path"])
    # write_jsonl(arkts_data, os.path.join(curdir, "../../out_rounds/judgements_partially_formatted_with_header.jsonl"))
