import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
from utils import *
import re
from typing import Dict, List, Optional
import hashlib
import json
from collections import defaultdict
from datasketch import MinHash, MinHashLSH
import tree_sitter_typescript as tst
from tree_sitter import Language, Parser
from tqdm import tqdm

# --- Part 1: Heuristic Filtering ---

# Configuration for heuristic-based filtering rules.
# Thresholds are based on best practices from projects like The Stack and Codex.
SEED = 42
MAX_AVG_LINE_LENGTH = 100
MAX_LINE_LENGTH = 1000
AUTO_GEN_KEYWORDS = [
    "auto-generated",
    # "autogenerated",
    # "do not edit",
    # "generated by",
]

tree_sitter_language = Language(tst.language_typescript())
parser = Parser()
parser.language = tree_sitter_language


def mask_private_key(text: str) -> str:
    """
    将 PEM 格式的私钥段内容替换成 <MASKED_PRIVATE_KEY> 占位符。
    支持 BEGIN/END PRIVATE KEY 以及 BEGIN/END RSA PRIVATE KEY。
    """
    pattern = (
        r'-----BEGIN (?:RSA )?PRIVATE KEY-----'  # 开头
        r'.*?'                                   # 任意内容（包括换行）
        r'-----END (?:RSA )?PRIVATE KEY-----'    # 结束
    )
    return re.sub(
        pattern,
        lambda m: "-----BEGIN PRIVATE KEY-----\n<MASKED_PRIVATE_KEY>\n-----END PRIVATE KEY-----",
        text,
        flags=re.DOTALL | re.IGNORECASE
    )

def auto_generated_filter(text: str) -> bool:
    """Filter out auto-generated files"""
    if not text:
        return False
    lower_content = text.lower()
    return not any(keyword in lower_content for keyword in AUTO_GEN_KEYWORDS)


def avg_line_length_filter(text: str, max_avg_length: int = MAX_AVG_LINE_LENGTH) -> bool:
    """Filter out records with excessive average line length"""
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    total_chars = sum(len(line) for line in lines)
    return (total_chars / len(lines)) <= max_avg_length


def max_line_length_filter(text: str, max_length: int = MAX_LINE_LENGTH) -> bool:
    """Filter out records with extremely long lines"""
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    return not any(len(line) > max_length for line in lines)


# --- Tree-sitter syntax error filter ---

def _count_ts_errors_and_missing(text: str):
    """
    统计 Tree-sitter 解析树中的 ERROR 节点数量与 missing 节点数量。
    返回: (error_nodes, missing_nodes, total_nodes)
    """
    if not text or not isinstance(text, str):
        return (0, 0, 0)

    tree = parser.parse(bytes(text, "utf8"))
    root = tree.root_node
    if root is None:
        return (0, 0, 0)

    err_cnt = 0
    miss_cnt = 0
    total = 0

    # DFS 遍历
    stack = [root]
    while stack:
        node = stack.pop()
        total += 1

        # 明确 ERROR 类型的节点
        if node.type == "ERROR":
            err_cnt += 1
        # 语法中缺失的必需构件
        if node.is_missing:
            miss_cnt += 1

        # 避免重复计算：has_error 只作为快速路径，不单独计数
        # if node.has_error: pass

        # 推入子节点
        # 用 children 而非 named_children，确保不漏掉结构性 token
        stack.extend(node.children)

    return (err_cnt, miss_cnt, total)


def tree_sitter_error_filter(
    text: str,
    max_error_nodes: int = 0,
    allow_missing: bool = False,
    max_error_ratio: float = 0.0,
):
    """
    语法错误过滤器：
    - 若 ERROR 节点数 > max_error_nodes，则过滤（返回 False）
    - 若不允许缺失且存在 missing 节点，则过滤
    - 若设置了 max_error_ratio 且 ERROR 节点占比超过阈值，也过滤
    返回 True 表示“保留”，False 表示“过滤掉”
    """
    err_cnt, miss_cnt, total = _count_ts_errors_and_missing(text)

    # 缺失节点控制
    if not allow_missing and miss_cnt > 0:
        return False

    # 绝对数量阈值
    if err_cnt > max_error_nodes:
        return False

    # 占比阈值（可选）
    if total > 0 and max_error_ratio > 0.0:
        if (err_cnt / total) > max_error_ratio:
            return False

    return True


def empty_content_filter(text: str) -> bool:
    """Filter out records with no content"""
    if not text:
        return False
    lines = text.splitlines()
    return bool(lines)


# --- Part 2: Exact Deduplication ---

def get_content_hash(content):
    """
    Computes the SHA256 hash of a string content.

    Args:
        content (str): The string content to hash.

    Returns:
        str: The hexadecimal SHA256 hash digest.
    """
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def find_exact_duplicates(records):
    """
    Finds records with exactly duplicate content using hashing.

    Args:
        records (list[dict]): A list of records to check for duplicates.
                               Each record must have 'id' and 'content'.

    Returns:
        set: A set of 'id's of the records to be removed.
    """
    hashes = defaultdict(list)
    for record in records:
        content_hash = get_content_hash(record['text'])
        hashes[content_hash].append(record['id'])

    to_remove = set()
    for hash_val, ids in hashes.items():
        if len(ids) > 1:
            # Keep the first one, mark the rest for removal
            to_remove.update(ids[1:])
            print(f"  - Found {len(ids)} exact duplicates. Keeping '{ids}', removing {ids[1:]}.")

    return to_remove


# --- Part 3: Near-Deduplication ---

# Configuration for MinHash and LSH parameters.
# Based on industry standards and recommendations from The Stack.
JACCARD_THRESHOLD = 0.85
NUM_PERMUTATIONS = 256


def find_near_duplicates(records):
    """
    Finds near-duplicate records using MinHash and LSH.

    Args:
        records (list[dict]): A list of records to check.

    Returns:
        set: A set of 'id's of the near-duplicate records to be removed.
    """
    print("\n Creating MinHash signatures...")

    # 1. Create a MinHash signature for each record
    minhashes = {}
    for i, record in enumerate(records):
        content = record.get("text", "")
        if not content:
            continue

        # Create shingles (5-grams of characters) from the content
        shingles = {content[i:i + 5] for i in range(len(content) - 4)}
        if not shingles:
            continue

        m = MinHash(num_perm=NUM_PERMUTATIONS, seed=SEED)
        for s in shingles:
            m.update(s.encode('utf8'))
        minhashes[record['id']] = m

        if (i + 1) % 200 == 0:
            print(f"  - Processed {i + 1}/{len(records)} records...")

    print(f" MinHash signatures created for {len(minhashes)} records.")
    print(" Building LSH index...")

    # 2. Index all MinHash signatures in the LSH model
    lsh = MinHashLSH(threshold=JACCARD_THRESHOLD, num_perm=NUM_PERMUTATIONS)
    for record_id, m in minhashes.items():
        lsh.insert(record_id, m)

    print(" LSH index built. Querying for duplicate clusters...")

    # 3. Query for duplicates and identify clusters
    processed_ids = set()
    clusters = []
    for record_id, m in minhashes.items():
        if record_id in processed_ids:
            continue

        # Find candidate pairs for the current record
        result = lsh.query(m)

        if len(result) > 1:
            cluster = set(result)
            clusters.append(list(cluster))
            processed_ids.update(cluster)

    # 4. From each cluster, keep one record and mark others for removal
    to_remove = set()
    for cluster in clusters:
        # A simple strategy: keep the record with the lexicographically smallest ID
        cluster.sort()
        kept_id = cluster[0]
        removed_ids = cluster[1:]
        to_remove.update(removed_ids)

        print(f"  - Cluster found. Keeping '{kept_id}', removing {removed_ids}")

    return to_remove

def remove_angle_bracket_headers(text: str) -> str:
    """
    Remove all lines containing <any content> format from the beginning of the code.
    Only processes consecutive lines containing angle bracket tags at the start of the file.
    
    Args:
        text: String containing the code
        
    Returns:
        Code string after removing all <any content> header information
    """
    lines = text.split('\n')
    # Find index of first line not containing <tag> format
    start_index = 0
    for i, line in enumerate(lines):
        # Check if line contains <tag> format (excluding cases within quotes)
        # Simple heuristic: if line starts with < or main content is <tag>, consider it header info
        stripped_line = line.strip()
        if not stripped_line:
            continue
        # If line contains <tag> and looks like header info (no common code syntax)
        if ('<' in line and '>' in line and 
            not any(keyword in line for keyword in ['=', '(', ')', '{', '}', ';', 'function', 'const', 'let', 'var', 'class', 'import', 'export']) and
            not line.strip().startswith('//')): # Not a comment
            start_index = i + 1
        else:
            break
    
    return '\n'.join(lines[start_index:])


def clean_code_headers(code: str) -> str:
    """
    Uses regular expressions to remove Copyright and Licensed comment blocks from the code.

    Args:
        code: A string containing the code.

    Returns:
        The code string after removing the copyright and license statements.
    """
    # This regular expression finds multi-line comment blocks (/* ... */)
    # containing the keywords "Copyright" or "Licensed"
    # The re.DOTALL flag allows '.' to match any character, including newlines.
    # The re.IGNORECASE flag makes the matching case-insensitive.
    # \s* will match and remove trailing whitespace (like newlines) after the comment block for cleaner formatting.
    pattern = r'/\*+.*?(?:Copyright|Licensed).*?\*/\s*'

    cleaned_code = re.sub(pattern, '', code, flags=re.DOTALL | re.IGNORECASE)

    return cleaned_code.lstrip()  # Remove potential leading blank lines created by deleting the comment.

def remove_single_line_comments(text: str) -> str:
    """Strip // comments"""
    return re.sub(r'//.*', '', text)

def remove_leading_blank_lines(text: str) -> str:
    """
    Remove blank lines (empty or whitespace-only) at the very beginning of the text.
    """
    if not isinstance(text, str) or not text:
        return text
    lines = text.split('\n')
    start_index = 0
    while start_index < len(lines) and lines[start_index].strip() == '':
        start_index += 1
    return '\n'.join(lines[start_index:])

def ensure_ts_header(text: str, path: Optional[str]) -> str:
    """
    Ensure the first non-empty line is a header like "// dir/file.ts" or "// dir/file.tsx".
    If missing and `path` is provided, prepend a header built from the last two segments of `path`.
    """
    if not isinstance(text, str):
        return text

    header_regex = re.compile(r'^\s*//\s*.+\.(?:ts|tsx)\s*$', re.IGNORECASE)
    lines = text.split('\n')

    # Find first non-empty line
    first_idx = 0
    while first_idx < len(lines) and lines[first_idx].strip() == '':
        first_idx += 1

    if first_idx < len(lines) and header_regex.match(lines[first_idx] or ''):
        return text

    if not path:
        return text

    norm_path = os.path.normpath(path)
    parts = [p for p in norm_path.split(os.sep) if p]
    if not parts:
        return text
    last_two = '/'.join([parts[-1]]) if len(parts) >= 2 else parts[-1]
    header_line = f"// {last_two}"
    return header_line + ('\n' if text and not text.startswith('\n') else '\n') + text

# --- New Filter Functions ---

def min_line_count_filter(text: str, min_lines: int = 5) -> bool:
    """Keep if text has at least min_lines non-empty lines"""
    content_lines = get_content_lines(text)
    return len(content_lines) >= min_lines


def blank_vs_content_filter(text: str, max_ratio: float = 0.8, min_blank: int = 14) -> bool:
    """Filter out if blank lines exceed content lines or ratio threshold"""
    lines = text.splitlines()
    content_lines = [l for l in lines if l.strip()]
    blank = len(lines) - len(content_lines)
    if blank > len(content_lines):
        return False
    if len(content_lines) > 0 and (blank / len(content_lines) > max_ratio and blank >= min_blank):
        return False
    return True


def colon_density_filter(text: str, threshold: float = 0.45, min_content: int = 80) -> bool:
    """Filter out ArkTS-heavy records with high colon density"""
    content_lines = [l for l in text.splitlines() if l.strip()]
    if len(content_lines) < min_content:
        return True
    density = sum(1 for l in content_lines if ':' in l) / len(content_lines)
    return density <= threshold


def trivial_assignment_filter(text: str, vars: List[str] = ['a','b','c']) -> bool:
    """Drop if trivial assignments like 'a =', 'b=' etc. appear"""
    pattern = rf"\b({'|'.join(vars)})\s*="
    return re.search(pattern, text) is None


def get_content_lines(text: str) -> List[str]:
    """Extract non-empty, non-comment lines from text"""
    return [line.strip() for line in text.splitlines() 
            if line.strip() and not line.strip().startswith('//')]

def prefix_repetition_filter(text: str, prefix_len: int = 16, max_density: float = 0.4) -> bool:
    """Ensure no prefix of length prefix_len repeats more than max_density ratio of content lines"""
    # Get non-empty, non-comment lines
    content_lines = get_content_lines(text)
    if not content_lines:
        return False
        
    # Count prefix occurrences
    counts = defaultdict(int)
    for line in content_lines:
        if len(line) <= prefix_len:
            continue
        prefix = line[:prefix_len]
        counts[prefix] += 1
    
    # Check if any prefix density exceeds threshold
    total_lines = len(content_lines)
    for count in counts.values():
        if count / total_lines > max_density:
            return False
            
    return True


def chinese_char_threshold_filter(text: str, threshold: float = 0.3) -> bool:
    """Drop if Chinese character ratio exceeds threshold"""
    # Get content lines to calculate valid characters
    content_lines = get_content_lines(text)
    if not content_lines:
        return True
    
    # Calculate total valid characters (excluding whitespace)
    total_chars = sum(len(line.replace(' ', '').replace('\t', '')) for line in content_lines)
    if total_chars == 0:
        return True
    
    # Count Chinese characters
    chinese_count = sum(1 for c in ''.join(content_lines) if '\u4e00' <= c <= '\u9fff')
    chinese_ratio = chinese_count / total_chars
    
    return chinese_ratio <= threshold


def numeric_literal_density_filter(text: str, max_density: float = 0.3) -> bool:
    """
    如果数值常量在所有 token 中占比超过 max_density，则过滤掉该 record。
    """
    # 匹配所有数值常量：整数或浮点数
    num_literals = re.findall(r'\b-?\d+(\.\d+)?\b', text)
    # 匹配所有单词 token（包括标识符、关键字、常量等）
    all_tokens = re.findall(r'\b\w+\b', text)
    if not all_tokens:
        return False
    density = len(num_literals) / len(all_tokens)
    return density <= max_density

def numeric_array_length_filter(text: str, max_len: int = 10) -> bool:
    """
    检测代码中所有 [...] 数组初始化：
    若数组内所有元素均为数值且元素数量 > max_len，则过滤掉该 record。
    """
    for match in re.finditer(r'\[([^\]]+)\]', text):
        elems = [e.strip() for e in match.group(1).split(',') if e.strip()]
        # 判断是否全部为数字字面量
        if elems and all(re.fullmatch(r'-?\d+(\.\d+)?', e) for e in elems):
            if len(elems) > max_len:
                return False
    return True

def total_length_filter(text: str, max_length: int = 30696) -> bool:
    """Filter out records with total character count exceeding max_length"""
    return len(text) <= max_length


def count_empty_functions(text: str):
    """
    返回 (total_functions, empty_functions)
    空函数定义为：函数体里只有大括号，没有任何子节点语句。
    """
    tree = parser.parse(bytes(text, 'utf8'))
    root = tree.root_node

    if root is None:
        return 0, 0

    total, empty = 0, 0
    # Tree-sitter 中常见的函数/方法节点类型，视语言而定可增删
    fn_types = {
        'function_declaration',
        'method_definition',
        'function',
        'arrow_function',
        'function_signature'  # 根据你用的 TypeScript 语法树可能要调整
    }

    # 使用显式栈进行 DFS，避免 cursor.node 的可空类型告警
    stack = [root]
    while stack:
        node = stack.pop()
        if getattr(node, 'type', None) in fn_types:
            total += 1
            # 找到直接子节点里的 `{ ... }`
            for c in getattr(node, 'children', []):
                if getattr(c, 'type', None) == 'statement_block':
                    non_brace_children = [
                        gc for gc in getattr(c, 'children', [])
                        if getattr(gc, 'type', None) not in ('{', '}')
                    ]
                    if len(non_brace_children) == 0:
                        empty += 1
                    break
        # 推入子节点
        stack.extend(getattr(node, 'children', []))

    return total, empty

# 预编译若干常见模板语法的正则
_RE_JINJA_STMT = re.compile(r"{%-?\s*[^%]+?-?%}")                 # {% ... %}
_RE_JINJA_EXPR = re.compile(r"{{-?\s*[^}]+?-?}}")                 # {{ ... }}
_RE_JINJA_CTRL = re.compile(r"{%-?\s*(for|if|elif|else|endif|endfor|block|endblock|extends|include|set|macro|endmacro)\b[^%]*-?%}")

_RE_MUSTACHE_SEC = re.compile(r"{{\s*[#/\^>]\s*[^}]+}}")          # {{#section}}, {{/section}}, {{^inverse}}, {{> partial}}
_RE_HANDLEBARS_TRIPLE = re.compile(r"{{{[^}]+}}}")                # {{{ ... }}}

_RE_EJS_ERB = re.compile(r"<%[\s\S]*?%>")                         # EJS/ERB <% ... %>

# 你示例中的“/*--- ... ---*/”块（YAML风注释）也作为信号
_RE_YAML_BLOCK_COMMENT = re.compile(r"/\*\s*---[\s\S]+?---\s*\*/")

def jinja_template_filter(code: str) -> bool:
    """
    检测代码片段是否包含常见模板语法（Jinja/Nunjucks/Mustache/Handlebars/EJS/ERB 等）。
    返回 True 表示“应过滤（疑似模板）”；False 表示“保留”。
    """
    if not code or not isinstance(code, str):
        return False

    # 高置信度命中：任何一种即可直接过滤
    if (_RE_JINJA_STMT.search(code) or
        _RE_JINJA_CTRL.search(code) or
        _RE_EJS_ERB.search(code) or
        _RE_MUSTACHE_SEC.search(code) or
        _RE_HANDLEBARS_TRIPLE.search(code) or
        _RE_YAML_BLOCK_COMMENT.search(code)):
        return True

    # 中等置信度：大量 {{ ... }} 表达式（但避免把普通语言误杀）
    # ArkTS/TS/JS 正常不会在代码里使用 {{}}（模板字符串是 ${}），
    # 若出现多处 {{...}}，可认为是模板。
    exprs = _RE_JINJA_EXPR.findall(code)
    if len(exprs) >= 2:
        return True

    # 低置信度补充：同时出现 {% 和 %} 也很可疑
    if code.count("{%") >= 1 and code.count("%}") >= 1:
        return True

    return False


def data_clean_pipeline(corpus: List[Dict], out_dir: str = "./filter_logs", preprocess_only: bool = False):
    """
    Cleans the input corpus by removing comments, filtering out empty texts,
    and ensuring that the text does not contain more than 5 Chinese characters.

    Args:
        corpus: A list of dictionaries, each containing a 'text' key with the code to be cleaned.
        out_dir: Directory to save filter logs.
        preprocess_only: If True, only run preprocessing steps (mask private keys, remove headers, clean code headers)
                        without applying filters and deduplication.

    Returns:
        A list of cleaned dictionaries.
    """
    os.makedirs(out_dir, exist_ok=True)
    removed = defaultdict(list)
    print("--- Step 0: Begin with {} records ---".format(len(corpus)))
    tmp_corpus = []

    ## Preprocess and normalize headers
    for item in corpus:
        text = item.get("text", "")
        # 1) Mask PrivateKey
        text = mask_private_key(text)
        # 2) Trim leading blank lines
        text = remove_leading_blank_lines(text)
        # 3) Remove angle bracket headers at the very beginning
        text = remove_angle_bracket_headers(text)
        # 4) Remove license/copyright blocks
        text = clean_code_headers(text)
        # 5) Trim leading blank lines again in case previous step introduced them
        text = remove_leading_blank_lines(text)
        # 6) Ensure // path/to/file.ts(x) header exists using last two segments of path
        text = ensure_ts_header(text, item.get("path"))

        item['text'] = text
        tmp_corpus.append(item)

    # If preprocess_only is True, skip all filtering and deduplication steps
    if preprocess_only:
        print("--- Preprocessing Only Mode: Skipping filters and deduplication ---")
        print(f"Returning {len(tmp_corpus)} preprocessed records.")
        return tmp_corpus

    print("--- Step 1: All Filters ---")
    aft = []
    for r in tmp_corpus:
        t = r['text']
        
        # Heuristic filters (拆分后的独立过滤器)
        if not empty_content_filter(t):
            removed['empty_content_filter'].append(r)
            continue
        if not auto_generated_filter(t):
            removed['auto_generated_filter'].append(r)
            continue
        if not avg_line_length_filter(t):
            removed['avg_line_length_filter'].append(r)
            continue
        if not max_line_length_filter(t):
            removed['max_line_length_filter'].append(r)
            continue
        # Additional filters
        # if not tree_sitter_error_filter(t, max_error_nodes=0, allow_missing=True, max_error_ratio=0.0):
        #     removed['tree_sitter_error_filter'].append(r)
        #     continue
        if not min_line_count_filter(t):
            removed['min_line_count_filter'].append(r)
            continue
        if not blank_vs_content_filter(t):
            removed['blank_vs_content_filter'].append(r)
            continue
        if not colon_density_filter(t):
            removed['colon_density_filter'].append(r)
            continue
        if not prefix_repetition_filter(t, prefix_len=16):
            removed['prefix_repetition_filter'].append(r)
            continue
        if not chinese_char_threshold_filter(t):
            removed['chinese_char_threshold_filter'].append(r)
            continue
        if not numeric_literal_density_filter(t):
            removed['numeric_literal_density_filter'].append(r)
            continue
        if not numeric_array_length_filter(t):
            removed['numeric_array_length_filter'].append(r)
            continue
        if not total_length_filter(t):
            removed['total_length_filter'].append(r)
            continue
        # if not jinja_template_filter(t):
        #     removed['jinja_template_filter'].append(r)
        #     continue
        aft.append(r)

    # 分别写入每个 filter 丢弃记录
    for name, recs in {
        "removed_empty_content_filter": removed['empty_content_filter'],
        "removed_auto_generated_filter": removed['auto_generated_filter'],
        "removed_avg_line_length_filter": removed['avg_line_length_filter'],
        "removed_max_line_length_filter": removed['max_line_length_filter'],
        "removed_min_line_count_filter": removed['min_line_count_filter'],
        "removed_blank_vs_content_filter": removed['blank_vs_content_filter'],
        "removed_colon_density_filter": removed['colon_density_filter'],
        "removed_prefix_repetition_filter": removed['prefix_repetition_filter'],
        "removed_chinese_char_threshold_filter": removed['chinese_char_threshold_filter'],
        "removed_numeric_literal_density_filter": removed['numeric_literal_density_filter'],
        "removed_numeric_array_length_filter": removed['numeric_array_length_filter'],
        "removed_total_length_filter": removed['total_length_filter'],
        # "removed_jinja_template_filter": removed['jinja_template_filter'],
        # "removed_tree_sitter_error_filter": removed['tree_sitter_error_filter'],
    }.items():
        write_jsonl(recs, os.path.join(out_dir, f"{name}.jsonl"))
        print(f"{name} 丢弃 {len(recs)} 条")

    print(f"Kept {len(aft)} after all filters.\n")

    # # Step 2: Exact Deduplication
    # print("--- Step 2: Applying Exact Deduplication ---")
    # exact_dup_ids = find_exact_duplicates(aft)
    # records_after_exact_dedup = [r for r in aft if r['id'] not in exact_dup_ids]
    # print(f"Exact deduplication complete. Kept {len(records_after_exact_dedup)} records.\n")

    # # Step 3: Near-Deduplication
    # print("--- Step 3: Applying Near-Deduplication ---")
    # near_dup_ids = find_near_duplicates(records_after_exact_dedup)
    # final_clean_records = [r for r in records_after_exact_dedup if r['id'] not in near_dup_ids]
    # print(f"Near-deduplication complete. Kept {len(final_clean_records)} records.\n")

    return aft



# --- Test Cases ---
if __name__ == "__main__":
    # Get the current working directory where the script is run from
    curdir = os.path.dirname(__file__)
    print(f"Running from directory: {curdir}")
    raw_data = read_jsonl(os.path.join(curdir, "../code_data/ts_data/split/formatted_starcode_ts_without_tag_train_sub30k.jsonl"))
    
    # Example 1: Run full pipeline with filtering and deduplication
    cleaned_data = data_clean_pipeline(raw_data)
    write_jsonl(cleaned_data, os.path.join(curdir, "../code_data/ts_data/split/ts_train_sub30k_cleaned_with_header.jsonl"))
    wo_header = read_jsonl("/data2/lyh/pretrain_etl/data_processing/code_data/ts_data/split/ts_val_upload.jsonl")
    for rec in wo_header:
        rec["text"] = ensure_ts_header(rec["text"], rec["path"])
    write_jsonl(wo_header, "/data2/lyh/pretrain_etl/data_processing/code_data/ts_data/split/ts_val_upload_with_header.jsonl")
    wo_header = read_jsonl("/data2/lyh/pretrain_etl/data_processing/code_data/ts_data/split/ts_test_upload.jsonl")
    for rec in wo_header:
        rec["text"] = ensure_ts_header(rec["text"], rec["path"])
    write_jsonl(wo_header, "/data2/lyh/pretrain_etl/data_processing/code_data/ts_data/split/ts_test_upload_with_header.jsonl")
    
    
