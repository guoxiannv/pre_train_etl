#!/usr/bin/env python3
"""
ä¸“é—¨ç”¨äºæ£€æŸ¥JSONLæ–‡ä»¶ä¸­textå­—æ®µè¯­æ³•é”™è¯¯çš„è„šæœ¬
å…è®¸3ä¸ªä»¥å†…çš„è¯­æ³•é”™è¯¯ï¼Œè¾“å‡ºè¯¦ç»†çš„è¯­æ³•é”™è¯¯æŠ¥å‘Š
"""

import json
import re
import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# Tree-sitterç›¸å…³å¯¼å…¥
try:
    import tree_sitter_typescript as tst
    from tree_sitter import Language, Parser
    TREE_SITTER_AVAILABLE = True
    print("âœ… Tree-sitterå¯ç”¨ï¼Œå°†è¿›è¡Œè¯­æ³•æ£€æŸ¥")
except ImportError:
    TREE_SITTER_AVAILABLE = False
    print("âŒ Tree-sitterä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install tree-sitter tree-sitter-typescript")
    sys.exit(1)


def setup_tree_sitter() -> Optional[Parser]:
    """è®¾ç½®Tree-sitterè§£æå™¨"""
    try:
        tree_sitter_language = Language(tst.language_typescript())
        parser = Parser()
        parser.language = tree_sitter_language
        return parser
    except Exception as e:
        print(f"âŒ Tree-sitterè®¾ç½®å¤±è´¥: {e}")
        return None


def normalize_text(text: str) -> str:
    """
    æ ‡å‡†åŒ–textå†…å®¹ï¼Œå¤„ç†æ¢è¡Œç¬¦å’Œè½¬ä¹‰ç¬¦
    
    Args:
        text: åŸå§‹textå†…å®¹
        
    Returns:
        æ ‡å‡†åŒ–åçš„textå†…å®¹
    """
    if not text or not isinstance(text, str):
        return ""
    
    try:
        # å¤„ç†è½¬ä¹‰å­—ç¬¦
        normalized = text
        
        # å¤„ç†å¸¸è§çš„è½¬ä¹‰åºåˆ—
        escape_mappings = {
            r'\\n': '\n',      # åŒåæ–œæ n -> æ¢è¡Œç¬¦
            r'\\t': '\t',      # åŒåæ–œæ t -> åˆ¶è¡¨ç¬¦
            r'\\r': '\r',      # åŒåæ–œæ r -> å›è½¦ç¬¦
            r'\\"': '"',       # åŒåæ–œæ å¼•å· -> å¼•å·
            r"\\'": "'",       # åŒåæ–œæ å•å¼•å· -> å•å¼•å·
            r'\\\\': '\\',     # åŒåæ–œæ  -> å•åæ–œæ 
        }
        
        for escaped, unescaped in escape_mappings.items():
            normalized = normalized.replace(escaped, unescaped)
        
        # å¤„ç†å…¶ä»–å¯èƒ½çš„è½¬ä¹‰åºåˆ—
        normalized = re.sub(r'\\(.)', r'\1', normalized)
        
        # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        normalized = normalized.replace('\r\n', '\n').replace('\r', '\n')
        
        # å»é™¤é¦–å°¾ç©ºç™½
        normalized = normalized.strip()
        
        return normalized
        
    except Exception as e:
        print(f"æ–‡æœ¬æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return text


def check_arkts_syntax(text: str, parser: Parser) -> Tuple[bool, int, int, int]:
    """
    ä½¿ç”¨Tree-sitteræ£€æŸ¥ArkTS/TypeScriptè¯­æ³•
    
    Args:
        text: è¦æ£€æŸ¥çš„ä»£ç æ–‡æœ¬
        parser: Tree-sitterè§£æå™¨
        
    Returns:
        (is_valid, error_count, missing_count, total_nodes)
    """
    if not parser or not text or not isinstance(text, str):
        return True, 0, 0, 0
    
    try:
        # è§£æä»£ç 
        tree = parser.parse(bytes(text, "utf8"))
        root = tree.root_node
        
        if root is None:
            return False, 0, 0, 0
        
        error_count = 0
        missing_count = 0
        total_nodes = 0
        
        # DFSéå†è¯­æ³•æ ‘
        stack = [root]
        while stack:
            node = stack.pop()
            total_nodes += 1
            
            # æ£€æŸ¥ERRORèŠ‚ç‚¹
            if node.type == "ERROR":
                error_count += 1
            
            # æ£€æŸ¥ç¼ºå¤±èŠ‚ç‚¹
            if node.is_missing:
                missing_count += 1
            
            # æ·»åŠ å­èŠ‚ç‚¹åˆ°æ ˆä¸­
            stack.extend(node.children)
        
        # åˆ¤æ–­è¯­æ³•æ˜¯å¦æ­£ç¡®ï¼ˆå…è®¸3ä¸ªä»¥å†…çš„é”™è¯¯ï¼‰
        is_valid = error_count <= 3 and missing_count == 0
        
        return is_valid, error_count, missing_count, total_nodes
        
    except Exception as e:
        print(f"è¯­æ³•æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False, 0, 0, 0


def process_jsonl_file(input_file: str, output_dir: str) -> dict:
    """
    å¤„ç†JSONLæ–‡ä»¶ï¼Œæ£€æŸ¥textå­—æ®µçš„è¯­æ³•é”™è¯¯ï¼Œæ ¹æ®é”™è¯¯æ•°é‡åˆ†ç±»è¾“å‡º
    
    Args:
        input_file: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        
    Returns:
        å¤„ç†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
    
    # è®¾ç½®Tree-sitter
    parser = setup_tree_sitter()
    if not parser:
        print("âŒ Tree-sitterè®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return {'total': 0, 'valid': 0, 'minor_errors': 0, 'major_errors': 0}
    
    # è¯»å–æ•°æ®
    try:
        data = []
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:  # è·³è¿‡ç©ºè¡Œ
                    continue
                
                try:
                    record = json.loads(line)
                    data.append(record)
                except json.JSONDecodeError as e:
                    print(f"ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
        
        print(f"æˆåŠŸè¯»å– {len(data)} æ¡è®°å½•")
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return {'total': 0, 'valid': 0, 'minor_errors': 0, 'major_errors': 0}
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {'total': 0, 'valid': 0, 'minor_errors': 0, 'major_errors': 0}
    
    # åˆ†ç±»å­˜å‚¨è®°å½•
    valid_records = []      # æ— è¯­æ³•é”™è¯¯
    minor_error_records = []  # è¯­æ³•é”™è¯¯å°‘äº3æ¡
    major_error_records = []  # è¯­æ³•é”™è¯¯è¶…è¿‡3æ¡
    error_details = []      # é”™è¯¯è¯¦æƒ…
    
    # å¤„ç†æ¯æ¡è®°å½•
    for i, record in enumerate(data):
        if 'text' not in record:
            print(f"è­¦å‘Šï¼šç¬¬ {i+1} æ¡è®°å½•ç¼ºå°‘ 'text' å­—æ®µ")
            # ç¼ºå°‘textå­—æ®µçš„è®°å½•å½’ç±»åˆ°major_errors
            major_error_records.append(record)
            error_details.append({
                'id': record.get('id', f'record_{i+1}'),
                'corpusid': record.get('corpusid', ''),
                'error_count': 0,
                'missing_count': 0,
                'total_nodes': 0,
                'error_type': 'ç¼ºå°‘textå­—æ®µ',
                'text_preview': ''
            })
            continue
        
        # æ ‡å‡†åŒ–textå†…å®¹
        original_text = record['text']
        normalized_text = normalize_text(original_text)
        record['normalized_text'] = normalized_text
        
        # è¯­æ³•æ£€æŸ¥
        if normalized_text:
            is_valid, error_count, missing_count, total_nodes = check_arkts_syntax(normalized_text, parser)
            
            record['syntax_valid'] = is_valid
            record['syntax_errors'] = error_count
            record['syntax_missing'] = missing_count
            record['total_nodes'] = total_nodes
            
            if is_valid:
                valid_records.append(record)
                record['error_details'] = "è¯­æ³•æ­£ç¡®"
            else:
                if error_count <= 3:
                    # è¯­æ³•é”™è¯¯å°‘äºç­‰äº3æ¡
                    minor_error_records.append(record)
                    record['error_details'] = f"è½»å¾®è¯­æ³•é”™è¯¯: {error_count}ä¸ªé”™è¯¯èŠ‚ç‚¹, {missing_count}ä¸ªç¼ºå¤±èŠ‚ç‚¹"
                else:
                    # è¯­æ³•é”™è¯¯è¶…è¿‡3æ¡
                    major_error_records.append(record)
                    record['error_details'] = f"ä¸¥é‡è¯­æ³•é”™è¯¯: {error_count}ä¸ªé”™è¯¯èŠ‚ç‚¹, {missing_count}ä¸ªç¼ºå¤±èŠ‚ç‚¹"
                
                # æ”¶é›†é”™è¯¯è®°å½•è¯¦æƒ…
                error_details.append({
                    'id': record.get('id', f'record_{i+1}'),
                    'corpusid': record.get('corpusid', ''),
                    'error_count': error_count,
                    'missing_count': missing_count,
                    'total_nodes': total_nodes,
                    'error_type': 'è¯­æ³•é”™è¯¯',
                    'text_preview': normalized_text[:200] + "..." if len(normalized_text) > 200 else normalized_text
                })
        else:
            # textå­—æ®µä¸ºç©º
            major_error_records.append(record)
            record['syntax_valid'] = False
            record['syntax_errors'] = 0
            record['syntax_missing'] = 0
            record['total_nodes'] = 0
            record['error_details'] = "textå­—æ®µä¸ºç©º"
            error_details.append({
                'id': record.get('id', f'record_{i+1}'),
                'corpusid': record.get('corpusid', ''),
                'error_count': 0,
                'missing_count': 0,
                'total_nodes': 0,
                'error_type': 'textå­—æ®µä¸ºç©º',
                'text_preview': ''
            })
        
        if (i + 1) % 100 == 0:
            print(f"å·²å¤„ç† {i + 1}/{len(data)} æ¡è®°å½•")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    input_path = Path(input_file)
    base_name = input_path.stem
    
    # ä¿å­˜æ— è¯­æ³•é”™è¯¯çš„è®°å½•
    if valid_records:
        valid_file = Path(output_dir) / f"{base_name}_valid.jsonl"
        try:
            with open(valid_file, 'w', encoding='utf-8') as f:
                for record in valid_records:
                    json.dump(record, f, ensure_ascii=False)
                    f.write('\n')
            print(f"âœ… æ— è¯­æ³•é”™è¯¯è®°å½•å·²ä¿å­˜åˆ°: {valid_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ— è¯­æ³•é”™è¯¯è®°å½•å¤±è´¥: {e}")
    
    # ä¿å­˜è½»å¾®è¯­æ³•é”™è¯¯çš„è®°å½•
    if minor_error_records:
        minor_error_file = Path(output_dir) / f"{base_name}_minor_errors.jsonl"
        try:
            with open(minor_error_file, 'w', encoding='utf-8') as f:
                for record in minor_error_records:
                    json.dump(record, f, ensure_ascii=False)
                    f.write('\n')
            print(f"âœ… è½»å¾®è¯­æ³•é”™è¯¯è®°å½•å·²ä¿å­˜åˆ°: {minor_error_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜è½»å¾®è¯­æ³•é”™è¯¯è®°å½•å¤±è´¥: {e}")
    
    # ä¿å­˜ä¸¥é‡è¯­æ³•é”™è¯¯çš„è®°å½•
    if major_error_records:
        major_error_file = Path(output_dir) / f"{base_name}_major_errors.jsonl"
        try:
            with open(major_error_file, 'w', encoding='utf-8') as f:
                for record in major_error_records:
                    json.dump(record, f, ensure_ascii=False)
                    f.write('\n')
            print(f"âœ… ä¸¥é‡è¯­æ³•é”™è¯¯è®°å½•å·²ä¿å­˜åˆ°: {major_error_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ä¸¥é‡è¯­æ³•é”™è¯¯è®°å½•å¤±è´¥: {e}")
    
    # ç”Ÿæˆé”™è¯¯è¯¦æƒ…æŠ¥å‘Š
    if error_details:
        error_report_file = Path(output_dir) / f"{base_name}_error_details.jsonl"
        try:
            with open(error_report_file, 'w', encoding='utf-8') as f:
                for error in error_details:
                    json.dump(error, f, ensure_ascii=False)
                    f.write('\n')
            print(f"âœ… é”™è¯¯è¯¦æƒ…æŠ¥å‘Šå·²ä¿å­˜åˆ°: {error_report_file}")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆé”™è¯¯è¯¦æƒ…æŠ¥å‘Šå¤±è´¥: {e}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(data),
        'valid': len(valid_records),
        'minor_errors': len(minor_error_records),
        'major_errors': len(major_error_records)
    }
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ¯ å¤„ç†å®Œæˆï¼")
    print(f"æ€»è®°å½•æ•°: {stats['total']}")
    print(f"æ— è¯­æ³•é”™è¯¯: {stats['valid']}")
    print(f"è½»å¾®è¯­æ³•é”™è¯¯(â‰¤3): {stats['minor_errors']}")
    print(f"ä¸¥é‡è¯­æ³•é”™è¯¯(>3): {stats['major_errors']}")
    
    return stats


def get_jsonl_files(input_dir: str) -> List[str]:
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        
    Returns:
        JSONLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    jsonl_files = []
    
    try:
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return jsonl_files
        
        # æŸ¥æ‰¾æ‰€æœ‰.jsonlæ–‡ä»¶
        for file_path in input_path.glob("*.jsonl"):
            jsonl_files.append(str(file_path))
        
        # ä¹ŸæŸ¥æ‰¾.jsonæ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for file_path in input_path.glob("*.json"):
            jsonl_files.append(str(file_path))
        
        print(f"åœ¨ {input_dir} ç›®å½•ä¸‹æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
        
    except Exception as e:
        print(f"è·å–JSONLæ–‡ä»¶åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    return jsonl_files


def batch_syntax_check(input_dir: str, output_dir: str) -> None:
    """
    æ‰¹é‡å¤„ç†trans_text_dataæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶ï¼Œè¿›è¡Œè¯­æ³•æ£€æŸ¥
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆtrans_text_dataï¼‰
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡è¯­æ³•æ£€æŸ¥...")
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)
    
    # è·å–æ‰€æœ‰JSONLæ–‡ä»¶
    jsonl_files = get_jsonl_files(input_dir)
    
    if not jsonl_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è¾“å…¥ç›®å½•")
        return
    
    # æ‰¹é‡å¤„ç†
    total_stats = {
        'total_files': len(jsonl_files),
        'total_records': 0,
        'total_valid': 0,
        'total_minor_errors': 0,
        'total_major_errors': 0
    }
    
    for i, input_file in enumerate(jsonl_files, 1):
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i}/{len(jsonl_files)}: {os.path.basename(input_file)}")
        
        # å¤„ç†æ–‡ä»¶
        stats = process_jsonl_file(input_file, output_dir)
        
        # ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
        total_stats['total_records'] += stats['total']
        total_stats['total_valid'] += stats['valid']
        total_stats['total_minor_errors'] += stats['minor_errors']
        total_stats['total_major_errors'] += stats['major_errors']
        
        print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {os.path.basename(input_file)}")
        print(f"   è®°å½•æ•°: {stats['total']}, æœ‰æ•ˆ: {stats['valid']}, è½»å¾®é”™è¯¯: {stats['minor_errors']}, ä¸¥é‡é”™è¯¯: {stats['major_errors']}")
    
    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ¯ æ‰¹é‡è¯­æ³•æ£€æŸ¥å®Œæˆï¼")
    print(f"æ€»æ–‡ä»¶æ•°: {total_stats['total_files']}")
    print(f"æ€»è®°å½•æ•°: {total_stats['total_records']}")
    print(f"æ€»æœ‰æ•ˆè®°å½•: {total_stats['total_valid']}")
    print(f"æ€»è½»å¾®é”™è¯¯è®°å½•: {total_stats['total_minor_errors']}")
    print(f"æ€»ä¸¥é‡é”™è¯¯è®°å½•: {total_stats['total_major_errors']}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ£€æŸ¥JSONLæ–‡ä»¶ä¸­textå­—æ®µçš„è¯­æ³•é”™è¯¯ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†')
    parser.add_argument('--input-dir', default='trans_text_data', help='è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: trans_text_dataï¼‰')
    parser.add_argument('--output-dir', default='syntax_check_results', help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: syntax_check_resultsï¼‰')
    parser.add_argument('--single-file', help='å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    curdir = os.path.dirname(__file__)
    
    if args.single_file:
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        if not Path(args.single_file).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.single_file}")
            return
        
        output_dir = Path(args.output_dir)
        if not output_dir.is_absolute():
            output_dir = Path(curdir) / output_dir
        
        os.makedirs(output_dir, exist_ok=True)
        stats = process_jsonl_file(args.single_file, str(output_dir))
        
    else:
        # æ‰¹é‡å¤„ç†
        input_dir = Path(args.input_dir)
        output_dir = Path(args.output_dir)
        
        if not input_dir.is_absolute():
            input_dir = Path(curdir) / input_dir
        
        if not output_dir.is_absolute():
            output_dir = Path(curdir) / output_dir
        
        if not input_dir.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            print("è¯·ç¡®ä¿trans_text_dataæ–‡ä»¶å¤¹å­˜åœ¨äºè„šæœ¬åŒçº§ç›®å½•ä¸‹")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        batch_syntax_check(str(input_dir), str(output_dir))


if __name__ == "__main__":
    main() 