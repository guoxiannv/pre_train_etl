#!/usr/bin/env python3
"""
æ‹¼æ¥JSONLæ–‡ä»¶ä¸­ä¸‰ä¸ªå­—æ®µçš„è„šæœ¬
å°† above_functions + source_method_code + below_functions æ‹¼æ¥æˆæ–°çš„ text å­—æ®µ
æ”¯æŒæ‰¹é‡å¤„ç† trans_text_data æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶
ä¸ºæ¯æ¡è®°å½•ç”ŸæˆåŸºäºfilePathçš„ç¨³å®šID
"""

import json
import os
import sys
import hashlib
from pathlib import Path
from typing import List, Dict


def concatenate_fields(record: Dict) -> str:
    """
    æ‹¼æ¥ä¸‰ä¸ªå­—æ®µï¼šabove_functions + source_method_code + below_functions
    
    Args:
        record: åŒ…å«å­—æ®µçš„å­—å…¸è®°å½•
        
    Returns:
        æ‹¼æ¥åçš„textå­—ç¬¦ä¸²
    """
    # è·å–ä¸‰ä¸ªå­—æ®µçš„å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ä¸ºç©ºå­—ç¬¦ä¸²
    above_functions = record.get('above_functions', '')
    source_method_code = record.get('source_method_code', '')
    below_functions = record.get('below_functions', '')
    
    # æ™ºèƒ½å¤„ç†å­—æ®µå€¼ï¼Œè½¬æ¢ä¸ºåˆé€‚çš„å­—ç¬¦ä¸²ï¼Œä¿ç•™åŸå§‹ç©ºæ ¼
    def process_field_value(value):
        if value is None:
            return ''
        elif isinstance(value, str):
            # ä¿ç•™åŸå§‹ç©ºæ ¼ï¼Œä¸ä½¿ç”¨strip()
            return value
        elif isinstance(value, (list, tuple)):
            # å¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œä¸”ä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            if len(value) == 0:
                return ''
            # å¦‚æœåˆ—è¡¨åŒ…å«å­—ç¬¦ä¸²å…ƒç´ ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥ï¼Œä¿ç•™æ¯ä¸ªå…ƒç´ çš„åŸå§‹ç©ºæ ¼
            elif all(isinstance(item, str) for item in value):
                return '\n'.join(item for item in value if item)  # ä¸ä½¿ç”¨strip()
            # å…¶ä»–æƒ…å†µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            else:
                return str(value)
        elif isinstance(value, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆå»é™¤é¦–å°¾çš„{}ï¼‰
            try:
                json_str = json.dumps(value, ensure_ascii=False)
                return json_str[1:-1] if json_str.startswith('{') and json_str.endswith('}') else json_str
            except:
                return str(value)
        else:
            # å…¶ä»–ç±»å‹ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return str(value)
    
    # å¤„ç†ä¸‰ä¸ªå­—æ®µ
    above_functions = process_field_value(above_functions)
    source_method_code = process_field_value(source_method_code)
    below_functions = process_field_value(below_functions)
    
    # ç›´æ¥æ‹¼æ¥ä¸‰ä¸ªå­—æ®µï¼Œä¿ç•™åŸæœ‰çš„ç©ºæ ¼å’Œæ¢è¡Œ
    concatenated_text = above_functions + source_method_code + below_functions
    
    return concatenated_text


def generate_stable_id(file_path: str) -> str:
    """
    åŸºäºfilePathç”Ÿæˆç¨³å®šçš„ID
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
        
    Returns:
        64ä½åå…­è¿›åˆ¶SHA256å“ˆå¸ŒID
    """
    if not file_path or not isinstance(file_path, str):
        # å¦‚æœæ²¡æœ‰filePathï¼Œç”Ÿæˆä¸€ä¸ªéšæœºID
        import uuid
        return str(uuid.uuid4())
    
    try:
        # ä½¿ç”¨SHA256å“ˆå¸Œç”Ÿæˆç¨³å®šID
        stable_id = hashlib.sha256(file_path.encode('utf-8')).hexdigest()
        return stable_id
    except Exception as e:
        print(f"ç”ŸæˆIDæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å‡ºé”™æ—¶ç”ŸæˆéšæœºID
        import uuid
        return str(uuid.uuid4())


def process_jsonl_file(input_file: str, output_dir: str) -> dict:
    """
    å¤„ç†å•ä¸ªJSONLæ–‡ä»¶ï¼Œæ‹¼æ¥å­—æ®µå¹¶ç”Ÿæˆæ–°çš„textå­—æ®µ
    åŒæ—¶ä¸ºæ¯æ¡è®°å½•ç”ŸæˆåŸºäºpathçš„ç¨³å®šIDï¼Œå¹¶æ ‡å‡†åŒ–å­—æ®µå
    
    Args:
        input_file: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        
    Returns:
        å¤„ç†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
    
    # è¯»å–æ•°æ®
    try:
        data = []
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:  # è·³è¿‡ç©ºè¡Œ
                    continue
                
                try:
                    record = json.loads(line)
                    data.append(record)
                except json.JSONDecodeError as e:
                    print(f"ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
        
        print(f"æˆåŠŸè¯»å– {len(data)} æ¡è®°å½•")
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return {'total': 0, 'processed': 0, 'skipped': 0, 'missing_fields': 0, 'id_generated': 0, 'fields_renamed': 0}
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {'total': 0, 'processed': 0, 'skipped': 0, 'missing_fields': 0, 'id_generated': 0, 'fields_renamed': 0}
    
    # å¤„ç†æ¯æ¡è®°å½•
    processed_count = 0
    skipped_count = 0
    missing_fields_count = 0
    id_generated_count = 0
    fields_renamed_count = 0
    
    for i, record in enumerate(data):
        # å­—æ®µåæ ‡å‡†åŒ–ï¼šå°†projectNameæ”¹ä¸ºproject_nameï¼Œå°†filePathæ”¹ä¸ºpath
        if 'projectName' in record:
            record['project_name'] = record.pop('projectName')
            fields_renamed_count += 1
            print(f"ç¬¬ {i+1} æ¡è®°å½•ï¼šå­—æ®µå 'projectName' å·²æ”¹ä¸º 'project_name'")
        
        if 'filePath' in record:
            record['path'] = record.pop('filePath')
            fields_renamed_count += 1
            print(f"ç¬¬ {i+1} æ¡è®°å½•ï¼šå­—æ®µå 'filePath' å·²æ”¹ä¸º 'path'")
        
        # ç”ŸæˆåŸºäºpathçš„ç¨³å®šID
        file_path = record.get('path', '')
        if file_path:
            record['id'] = generate_stable_id(file_path)
            id_generated_count += 1
            print(f"ç¬¬ {i+1} æ¡è®°å½•ï¼šåŸºäºè·¯å¾„ '{file_path}' ç”ŸæˆID: {record['id'][:8]}...")
        else:
            print(f"è­¦å‘Šï¼šç¬¬ {i+1} æ¡è®°å½•ç¼ºå°‘ 'path' å­—æ®µï¼Œå°†ç”ŸæˆéšæœºID")
            record['id'] = generate_stable_id("")
            id_generated_count += 1
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€éœ€çš„å­—æ®µ
        required_fields = ['above_functions', 'source_method_code', 'below_functions']
        missing_fields = [field for field in required_fields if field not in record]
        
        if missing_fields:
            print(f"è­¦å‘Šï¼šç¬¬ {i+1} æ¡è®°å½•ç¼ºå°‘å­—æ®µ: {missing_fields}")
            record['text'] = ""
            record['concatenation_status'] = f"ç¼ºå°‘å­—æ®µ: {', '.join(missing_fields)}"
            missing_fields_count += 1
            continue
        
        # æ‹¼æ¥å­—æ®µ
        try:
            concatenated_text = concatenate_fields(record)
            record['text'] = concatenated_text
            record['concatenation_status'] = "æˆåŠŸæ‹¼æ¥"
            processed_count += 1
        except Exception as e:
            print(f"è­¦å‘Šï¼šç¬¬ {i+1} æ¡è®°å½•æ‹¼æ¥å¤±è´¥: {e}")
            record['text'] = ""
            record['concatenation_status'] = f"æ‹¼æ¥å¤±è´¥: {str(e)}"
            skipped_count += 1
        
        if (i + 1) % 100 == 0:
            print(f"å·²å¤„ç† {i + 1}/{len(data)} æ¡è®°å½•")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    input_path = Path(input_file)
    base_name = input_path.stem
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    output_file = Path(output_dir) / f"{base_name}_concatenated.jsonl"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            for record in data:
                json.dump(record, f, ensure_ascii=False)
                f.write('\n')
        print(f"âœ… å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return {'total': len(data), 'processed': 0, 'skipped': 0, 'missing_fields': 0, 'id_generated': 0}
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(data),
        'processed': processed_count,
        'skipped': skipped_count,
        'missing_fields': missing_fields_count,
        'id_generated': id_generated_count,
        'fields_renamed': fields_renamed_count
    }
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ¯ å¤„ç†å®Œæˆï¼")
    print(f"æ€»è®°å½•æ•°: {stats['total']}")
    print(f"æˆåŠŸæ‹¼æ¥: {stats['processed']}")
    print(f"æ‹¼æ¥å¤±è´¥: {stats['skipped']}")
    print(f"ç¼ºå°‘å­—æ®µ: {stats['missing_fields']}")
    print(f"ç”ŸæˆID: {stats['id_generated']}")
    print(f"å­—æ®µé‡å‘½å: {stats['fields_renamed']}")
    
    return stats


def get_jsonl_files(input_dir: str) -> List[str]:
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        
    Returns:
        JSONLæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    jsonl_files = []
    
    try:
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return jsonl_files
        
        # æŸ¥æ‰¾æ‰€æœ‰.jsonlæ–‡ä»¶
        for file_path in input_path.glob("*.jsonl"):
            jsonl_files.append(str(file_path))
        
        # ä¹ŸæŸ¥æ‰¾.jsonæ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for file_path in input_path.glob("*.json"):
            jsonl_files.append(str(file_path))
        
        print(f"åœ¨ {input_dir} ç›®å½•ä¸‹æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
        
    except Exception as e:
        print(f"è·å–JSONLæ–‡ä»¶åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    return jsonl_files


def batch_process_files(input_dir: str, output_dir: str) -> None:
    """
    æ‰¹é‡å¤„ç†bg_data_selectæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆbg_data_selectï¼‰
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡æ‹¼æ¥å­—æ®µ...")
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)
    
    # è·å–æ‰€æœ‰JSONLæ–‡ä»¶
    jsonl_files = get_jsonl_files(input_dir)
    
    if not jsonl_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONLæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è¾“å…¥ç›®å½•")
        return
    
    # æ‰¹é‡å¤„ç†
    total_stats = {
        'total_files': len(jsonl_files),
        'total_records': 0,
        'total_processed': 0,
        'total_skipped': 0,
        'total_missing_fields': 0,
        'total_id_generated': 0,
        'total_fields_renamed': 0
    }
    
    for i, input_file in enumerate(jsonl_files, 1):
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i}/{len(jsonl_files)}: {os.path.basename(input_file)}")
        
        # å¤„ç†æ–‡ä»¶
        stats = process_jsonl_file(input_file, output_dir)
        
        # ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
        total_stats['total_records'] += stats['total']
        total_stats['total_processed'] += stats['processed']
        total_stats['total_skipped'] += stats['skipped']
        total_stats['total_missing_fields'] += stats['missing_fields']
        total_stats['total_id_generated'] += stats['id_generated']
        total_stats['total_fields_renamed'] += stats['fields_renamed']
        
        print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {os.path.basename(input_file)}")
        print(f"   è®°å½•æ•°: {stats['total']}, æˆåŠŸ: {stats['processed']}, å¤±è´¥: {stats['skipped']}, ç¼ºå°‘å­—æ®µ: {stats['missing_fields']}, ç”ŸæˆID: {stats['id_generated']}, å­—æ®µé‡å‘½å: {stats['fields_renamed']}")
    
    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ¯ æ‰¹é‡æ‹¼æ¥å®Œæˆï¼")
    print(f"æ€»æ–‡ä»¶æ•°: {total_stats['total_files']}")
    print(f"æ€»è®°å½•æ•°: {total_stats['total_records']}")
    print(f"æ€»æˆåŠŸæ‹¼æ¥: {total_stats['total_processed']}")
    print(f"æ€»æ‹¼æ¥å¤±è´¥: {total_stats['total_skipped']}")
    print(f"æ€»ç¼ºå°‘å­—æ®µ: {total_stats['total_missing_fields']}")
    print(f"æ€»ç”ŸæˆID: {total_stats['total_id_generated']}")
    print(f"æ€»å­—æ®µé‡å‘½å: {total_stats['total_fields_renamed']}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‹¼æ¥JSONLæ–‡ä»¶ä¸­çš„ä¸‰ä¸ªå­—æ®µï¼šabove_functions + source_method_code + below_functionsï¼ŒåŒæ—¶ç”ŸæˆåŸºäºpathçš„ç¨³å®šIDï¼Œå¹¶æ ‡å‡†åŒ–å­—æ®µå')
    parser.add_argument('--input-dir', default='bg_data_select', help='è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: bg_data_selectï¼‰')
    parser.add_argument('--output-dir', default='trans_text_data', help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: trans_text_dataï¼‰')
    parser.add_argument('--single-file', help='å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    curdir = os.path.dirname(__file__)
    
    if args.single_file:
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        if not Path(args.single_file).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.single_file}")
            return
        
        output_dir = Path(args.output_dir)
        if not output_dir.is_absolute():
            output_dir = Path(curdir) / output_dir
        
        os.makedirs(output_dir, exist_ok=True)
        stats = process_jsonl_file(args.single_file, str(output_dir))
        
    else:
        # æ‰¹é‡å¤„ç†
        input_dir = Path(args.input_dir)
        output_dir = Path(args.output_dir)
        
        if not input_dir.is_absolute():
            input_dir = Path(curdir) / input_dir
        
        if not output_dir.is_absolute():
            output_dir = Path(curdir) / output_dir
        
        if not input_dir.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            print("è¯·ç¡®ä¿bg_data_selectæ–‡ä»¶å¤¹å­˜åœ¨äºè„šæœ¬åŒçº§ç›®å½•ä¸‹")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        batch_process_files(str(input_dir), str(output_dir))


if __name__ == "__main__":
    main() 