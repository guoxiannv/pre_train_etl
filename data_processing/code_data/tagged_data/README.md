# Data Processing Pipeline

è¿™ä¸ªpipelineç”¨äºå¤„ç†`judgements.jsonl`æ–‡ä»¶ï¼ŒåŒ…å«æ•°æ®åˆ†ç¦»ã€æ³„éœ²æ£€æµ‹ã€tokenç»Ÿè®¡ç­‰æ­¥éª¤ã€‚

## åŠŸèƒ½ç‰¹æ€§

### ğŸš€ å®Œæ•´çš„å¤„ç†æµç¨‹
1. **æ•°æ®åˆ†ç¦»** - æ ¹æ®decisionå­—æ®µåˆ†ç¦»æ•°æ®ï¼Œç§»é™¤item_id
2. **æ³„éœ²æ£€æµ‹** - ä½¿ç”¨`remove_leaked.py`ä¸­çš„é€»è¾‘æ£€æµ‹æ•°æ®æ³„éœ²
3. **Tokenç»Ÿè®¡** - ä½¿ç”¨`calculate_tokens.py`ç»Ÿè®¡æ¯è¡Œçš„tokenæ•°é‡
4. **æ•°æ®åˆ†å‰²** - æ ¹æ®æ³„éœ²æ ‡è®°åˆ›å»ºtrain/test split
5. **æ‰©å±•é¢„ç•™** - ä¸ºæœªæ¥çš„pipelineæ­¥éª¤é¢„ç•™ç©ºé—´

### ğŸ”§ å¯å¤ç”¨ç»„ä»¶
- `tag_llm_judgements_with_leaks()` - ä»`remove_leaked.py`æŠ½å–çš„æ³„éœ²æ£€æµ‹å‡½æ•°
- è¾“å…¥ï¼š`List[Dict[str, Any]]` å’Œ `leaked_set_norm: set`
- è¾“å‡ºï¼šæ·»åŠ äº†`leaked: bool`å­—æ®µçš„`List[Dict[str, Any]]`

### ğŸ“ è¾“å‡ºæ–‡ä»¶
- `judgements.jsonl` - å¤„ç†åçš„åŸå§‹æ•°æ®ï¼ˆç§»é™¤item_idï¼ŒæŒ‰project_nameæ’åºï¼‰
- `removed_judgements.jsonl` - æ ‡è®°ä¸ºREMOVEçš„æ•°æ®
- `kept_with_tag_judgements.jsonl` - æ ‡è®°ä¸ºKEEP_WITH_TAGçš„æ•°æ®
- `kept_all_judgements.jsonl` - æ‰€æœ‰ä¿ç•™çš„æ•°æ®ï¼ˆKEEP + KEEP_WITH_TAGï¼‰
- `kept_all_leaked_tagged.jsonl` - æ·»åŠ äº†leakedå­—æ®µçš„æ•°æ®
- `kept_all_final.jsonl` - åŒ…å«leakedå’Œtext_tokenså­—æ®µçš„æ•°æ®
- `kept_all_with_split.jsonl` - æœ€ç»ˆç»“æœï¼ŒåŒ…å«splitå­—æ®µï¼Œtrainæ•°æ®åœ¨å‰ï¼Œtestæ•°æ®åœ¨å

## ä½¿ç”¨æ–¹æ³•

### 1. ç›´æ¥è¿è¡Œpipeline
```bash
cd data_processing/code_data/tagged_data
python format_outrounds.py
```

### 2. åœ¨å…¶ä»–è„šæœ¬ä¸­ä½¿ç”¨
```python
from format_outrounds import DataProcessingPipeline

# åˆ›å»ºpipelineå®ä¾‹
pipeline = DataProcessingPipeline("/path/to/workspace/root")

# è¿è¡Œå®Œæ•´pipeline
final_data = pipeline.run_pipeline()

# æˆ–è€…è¿è¡Œå•ä¸ªæ­¥éª¤
separated_data = pipeline.step1_separate_data()
leaked_data = pipeline.step2_detect_leaks(separated_data['kept_all'])
final_data = pipeline.step3_count_tokens(leaked_data)
```

### 3. æµ‹è¯•pipeline
```bash
cd data_processing/code_data/tagged_data
python test_pipeline.py
```

## Pipelineæ­¥éª¤è¯¦è§£

### Step 1: æ•°æ®åˆ†ç¦» (`step1_separate_data`)
- è¯»å–`out_rounds/judgements.jsonl`
- ç§»é™¤`item_id`å­—æ®µ
- æŒ‰`project_name`æ’åº
- æ ¹æ®`decision`å­—æ®µåˆ†ç¦»æ•°æ®

### Step 2: æ³„éœ²æ£€æµ‹ (`step2_detect_leaks`)
- åŠ è½½æ³„éœ²ä»“åº“åˆ—è¡¨
- ä¸ºæ¯ä¸ªæ¡ç›®æ·»åŠ `leaked: bool`å­—æ®µ
- ä½¿ç”¨è·¯å¾„åŒ¹é…ç®—æ³•æ£€æµ‹æ³„éœ²

### Step 3: Tokenç»Ÿè®¡ (`step3_count_tokens`)
- ä½¿ç”¨Qwen2.5-Coder-7B tokenizer
- ä¸ºæ¯ä¸ªæ¡ç›®æ·»åŠ `text_tokens: int`å­—æ®µ
- ç»Ÿè®¡æ€»tokenæ•°å’Œå¹³å‡å€¼

### Step 4: æ•°æ®åˆ†å‰² (`step4_make_split`)
- æ ¹æ®`leaked`å­—æ®µå°†æ•°æ®åˆ†ä¸ºtrainå’Œtestä¸¤ä¸ªsplit
- ä¸ºæ¯ä¸ªæ¡ç›®æ·»åŠ `split: str`å­—æ®µï¼ˆ'train' æˆ– 'test'ï¼‰
- é‡æ–°ç»„ç»‡æ•°æ®ï¼štrainæ•°æ®åœ¨å‰ï¼Œtestæ•°æ®åœ¨å
- è®¡ç®—å¹¶æ˜¾ç¤ºæ¯ä¸ªsplitçš„tokenç»Ÿè®¡ä¿¡æ¯
- è¾“å‡ºæœ€ç»ˆçš„åˆ†å‰²æ•°æ®æ–‡ä»¶

## ä¾èµ–è¦æ±‚

- `transformers` - ç”¨äºåŠ è½½tokenizer
- `tqdm` - è¿›åº¦æ¡æ˜¾ç¤º
- `pathlib` - è·¯å¾„å¤„ç†
- è‡ªå®šä¹‰æ¨¡å—ï¼š
  - `utils.py` - åŸºç¡€å·¥å…·å‡½æ•°
  - `remove_leaked.py` - æ³„éœ²æ£€æµ‹é€»è¾‘
  - `calculate_tokens.py` - tokenç»Ÿè®¡åŠŸèƒ½

## é…ç½®è¯´æ˜

### ä»£ç†è®¾ç½®
pipelineä¼šè‡ªåŠ¨é…ç½®ä»£ç†ç¯å¢ƒå˜é‡ï¼š
- `HTTP_PROXY=http://127.0.0.1:10809`
- `HTTPS_PROXY=http://127.0.0.1:10809`

### è·¯å¾„é…ç½®
æ‰€æœ‰è·¯å¾„éƒ½æ˜¯ç›¸å¯¹äºworkspace rootçš„ç›¸å¯¹è·¯å¾„ï¼Œpipelineä¼šè‡ªåŠ¨å¤„ç†è·¯å¾„è½¬æ¢ã€‚

## é”™è¯¯å¤„ç†

- å¦‚æœæ³„éœ²æ£€æµ‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰€æœ‰æ¡ç›®ä¼šè¢«æ ‡è®°ä¸ºéæ³„éœ²
- å¦‚æœtokenç»Ÿè®¡å¤±è´¥ï¼Œä¼šè¿”å›åŸå§‹æ•°æ®
- æ¯ä¸ªæ­¥éª¤éƒ½æœ‰è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œå¼‚å¸¸å¤„ç†

## æ‰©å±•æŒ‡å—

è¦æ·»åŠ æ–°çš„pipelineæ­¥éª¤ï¼š

1. åœ¨`DataProcessingPipeline`ç±»ä¸­æ·»åŠ æ–°çš„æ–¹æ³•
2. åœ¨`run_pipeline`æ–¹æ³•ä¸­è°ƒç”¨æ–°æ­¥éª¤
3. æ›´æ–°è¾“å‡ºè·¯å¾„é…ç½®
4. æ·»åŠ ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹

ç¤ºä¾‹ï¼š
```python
def step5_quality_filter(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Step 5: Quality filtering"""
    # å®ç°è´¨é‡è¿‡æ»¤é€»è¾‘
    return filtered_data
```

## æ³¨æ„äº‹é¡¹

- ç¡®ä¿è¾“å…¥æ–‡ä»¶`out_rounds/judgements.jsonl`å­˜åœ¨
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨è¾“å‡ºæ–‡ä»¶
- Tokenç»Ÿè®¡å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå–å†³äºæ•°æ®é‡
- å»ºè®®åœ¨è¿è¡Œå‰å¤‡ä»½é‡è¦æ•°æ®
